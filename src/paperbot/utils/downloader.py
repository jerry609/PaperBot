# securipaperbot/utils/downloader.py

from typing import Dict, List, Any, Optional
import aiohttp
import asyncio
from pathlib import Path
import urllib.parse
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime
import logging

# ä½¿ç”¨æ ‡å‡†æ—¥å¿—ï¼Œé¿å…ç›¸å¯¹å¯¼å…¥é—®é¢˜
def setup_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å·¥å…·ç±» - ä¼˜åŒ–ç‰ˆæœ¬"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = setup_logger(__name__)
        self.download_path = Path(self.config.get('download_path', './papers'))
        self.download_path.mkdir(parents=True, exist_ok=True)

        # é…ç½®ä¸‹è½½é‡è¯•å‚æ•° - ä¿å®ˆç¨³å®šçš„å‚æ•°
        self.max_retries = self.config.get('max_retries', 3)
        self.retry_delay = self.config.get('retry_delay', 3)  # å¢åŠ åˆ°3ç§’

        # å®Œå…¨å…³é—­å¹¶å‘ - ä½¿ç”¨å•çº¿ç¨‹ç¡®ä¿ç¨³å®šæ€§
        max_concurrent = 1  # å¼ºåˆ¶è®¾ä¸º1ï¼Œä¸ä½¿ç”¨å¹¶å‘
        self.semaphore = asyncio.Semaphore(max_concurrent)

        # ä¼šè®®URLæ¨¡æ¿
        self.conference_urls = {
            'ccs': 'https://dl.acm.org/doi/proceedings/10.1145/',
            'sp': 'https://ieeexplore.ieee.org/xpl/conhome/',
            'ndss': 'https://www.ndss-symposium.org/',
            'usenix': 'https://www.usenix.org/conference/'
        }

    async def download_paper(self, url: str, title: str, paper_index: int = 0, total_papers: int = 0) -> Dict[str, Any]:
        """ä¸‹è½½å•ç¯‡è®ºæ–‡ - ä¼˜åŒ–ç‰ˆæœ¬"""
        async with self.semaphore:
            try:
                # ç”Ÿæˆæ–‡ä»¶å
                safe_title = self._sanitize_filename(title)
                file_path = self.download_path / f"{safe_title}.pdf"

                # æ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼ˆä¸NDSS/USENIXä¿æŒä¸€è‡´ï¼‰
                if total_papers > 0:
                    progress = (paper_index + 1) / total_papers * 100
                    print(f"ğŸ’¾ [{paper_index+1}/{total_papers}] ä¸‹è½½: {title[:50]}{'...' if len(title) > 50 else ''}")

                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½å¹¶éªŒè¯æ–‡ä»¶
                if file_path.exists():
                    # éªŒè¯æ–‡ä»¶å¤§å°ï¼Œè¿‡å°çš„æ–‡ä»¶å¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                    file_size = file_path.stat().st_size
                    if file_size > 1024:  # å¤§äº1KBè®¤ä¸ºæœ‰æ•ˆ
                        return {
                            'success': True,
                            'path': str(file_path),
                            'cached': True,
                            'size': file_size
                        }
                    else:
                        # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                        file_path.unlink()
                        self.logger.warning(f"Removed invalid cached file: {file_path}")

                # ä¸‹è½½è®ºæ–‡
                content = await self._download_with_retry(url)
                if content:
                    # éªŒè¯ä¸‹è½½å†…å®¹
                    if len(content) < 1024:
                        raise Exception(f"Downloaded content too small ({len(content)} bytes), likely an error page")
                    
                    # ä¿å­˜æ–‡ä»¶
                    file_path.write_bytes(content)
                    file_size = len(content)

                    return {
                        'success': True,
                        'path': str(file_path),
                        'cached': False,
                        'size': file_size
                    }
                else:
                    raise Exception("Failed to download paper - no content received")

            except Exception as e:
                self.logger.error(f"Error downloading paper {title}: {str(e)}")
                return {
                    'success': False,
                    'error': str(e)
                }

    async def get_conference_papers(self, conference: str, year: str) -> List[Dict[str, Any]]:
        """è·å–ä¼šè®®è®ºæ–‡åˆ—è¡¨ - å¸¦è¿›åº¦æ˜¾ç¤º"""
        try:
            if conference not in self.conference_urls:
                raise ValueError(f"Unsupported conference: {conference}")

            base_url = self.conference_urls[conference]
            papers = []
            
            print(f"ğŸ” æ­£åœ¨è·å– {conference.upper()} {year} è®ºæ–‡åˆ—è¡¨...")

            # æ ¹æ®ä¼šè®®ç±»å‹é€‰æ‹©ç›¸åº”çš„è§£ææ–¹æ³•
            if conference == 'ccs':
                papers = await self._parse_ccs_papers(base_url, year)
            elif conference == 'sp':
                papers = await self._parse_sp_papers(base_url, year)
            elif conference == 'ndss':
                papers = await self._parse_ndss_papers(base_url, year)
            elif conference == 'usenix':
                papers = await self._parse_usenix_papers(base_url, year)
            
            if papers:
                print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                print(f"ğŸ“‹ æ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨:")
                for i, paper in enumerate(papers[:10]):
                    title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]
                    print(f"  {i+1:2d}. {title}{'...' if len(paper.get('title', '')) > 60 else ''}")
                
                if len(papers) > 10:
                    print(f"  ... å’Œå…¶ä»– {len(papers) - 10} ç¯‡è®ºæ–‡")
                
                # å¼€å§‹PDFé“¾æ¥éªŒè¯ä¸è¿›åº¦æ˜¾ç¤º
                print(f"\nğŸ”— æ­£åœ¨éªŒè¯PDFé“¾æ¥æœ‰æ•ˆæ€§...")
                valid_count = 0
                
                for i, paper in enumerate(papers):
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = (i + 1) / len(papers) * 100
                    progress_bar = 'â–ˆ' * int(progress // 5) + 'â–‘' * (20 - int(progress // 5))
                    print(f"\rğŸ“‹ [è¿›åº¦: {progress_bar}] {progress:.1f}% ({i+1}/{len(papers)}) éªŒè¯: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:30]}...", end='', flush=True)
                    
                    # æ£€æŸ¥URLæœ‰æ•ˆæ€§
                    if isinstance(paper.get('url'), str) and paper['url'].strip():
                        valid_count += 1
                
                print(f"\nâœ… PDFé“¾æ¥éªŒè¯å®Œæˆ: {valid_count}/{len(papers)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")

            return papers

        except Exception as e:
            self.logger.error(f"Error getting papers for {conference} {year}: {str(e)}")
            raise


    


    async def _parse_ndss_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è§£æNDSSè®ºæ–‡åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬å¸¦è¿›åº¦æ˜¾ç¤º"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        url = f"{base_url}ndss{full_year}/accepted-papers/"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        enhanced_timeout = aiohttp.ClientTimeout(total=120, connect=30, sock_read=60)
        
        print(f"ğŸŒ è®¿é—® NDSS {year} ä¼šè®®é¡µé¢...")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with aiohttp.ClientSession(headers=headers, timeout=enhanced_timeout) as session:
                    
                    async with session.get(url) as response:
                        print(f"âš¡ å°è¯• {attempt + 1}/{max_retries}: HTTP {response.status}")
                        
                        if response.status == 200:
                            print(f"ğŸ“ æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # æŸ¥æ‰¾NDSSè®ºæ–‡å®¹å™¨
                            paper_containers = soup.find_all('div', class_='tag-box rel-paper')
                            print(f"ğŸ“š æ‰¾åˆ° {len(paper_containers)} ä¸ªè®ºæ–‡å®¹å™¨")
                            
                            if not paper_containers:
                                print(f"âš ï¸  æœªæ‰¾åˆ°è®ºæ–‡å®¹å™¨ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
                                # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                                paper_containers = soup.find_all('div', class_='paper') or soup.find_all('article')
                                print(f"ğŸ”„ å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(paper_containers)} ä¸ªå®¹å™¨")
                            
                            # å¤„ç†è®ºæ–‡å®¹å™¨å¹¶æ˜¾ç¤ºç®€å•è¿›åº¦
                            for idx, container in enumerate(paper_containers):
                                if idx % 5 == 0 or idx == len(paper_containers) - 1:  # æ¯5ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                    progress = (idx + 1) / len(paper_containers) * 100
                                    print(f"ğŸ” è§£æè¿›åº¦: {progress:.1f}% ({idx+1}/{len(paper_containers)})")
                                
                                try:
                                    # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
                                    title_elem = (container.find('h3', class_='blog-post-title') or 
                                                 container.find('h3') or 
                                                 container.find('h2') or 
                                                 container.find('h1'))
                                    
                                    if not title_elem:
                                        continue
                                    
                                    title = title_elem.get_text().strip()
                                    
                                    # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                                    print(f"ğŸ“„ [{idx+1}/{len(paper_containers)}] æ‰¾åˆ°è®ºæ–‡: {title[:70]}{'...' if len(title) > 70 else ''}")
                                    
                                    # æå–ä½œè€…ä¿¡æ¯
                                    author_elem = container.find('p')
                                    authors_text = author_elem.get_text().strip() if author_elem else ''
                                    authors = [author.strip() for author in authors_text.split(',')] if authors_text else []
                                    
                                    # æå–è¯¦æƒ…é¡µé“¾æ¥
                                    detail_link = (container.find('a', class_='paper-link-abs') or 
                                                  container.find('a', href=True))
                                    detail_url = detail_link.get('href') if detail_link else ''
                                    
                                    # æå–PDFé“¾æ¥
                                    pdf_url = ''
                                    if detail_url:
                                        pdf_url = await self._get_ndss_pdf_from_detail_page(session, detail_url)
                                    
                                    paper_info = {
                                        'title': title,
                                        'authors': authors,
                                        'abstract': '',
                                        'url': pdf_url,
                                        'detail_url': detail_url,
                                        'doi': ''
                                    }
                                    
                                    if title and len(title) > 10:
                                        papers.append(paper_info)
                                        
                                except Exception as e:
                                    self.logger.warning(f"Error parsing paper container {idx}: {str(e)}")
                                    continue
                            
                            print(f"\nâœ… åŸºç¡€ä¿¡æ¯è§£æå®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
                            return papers
                            
                        elif response.status == 404:
                            print(f"âŒ NDSS {year} é¡µé¢ä¸å­˜åœ¨")
                            return []
                        else:
                            print(f"âš ï¸  HTTP {response.status}ï¼Œæ­£åœ¨é‡è¯•...")
                            if attempt < max_retries - 1:
                                await asyncio.sleep(2 ** attempt)
                                continue
                            else:
                                raise Exception(f"HTTP {response.status}")
            
            except asyncio.TimeoutError:
                print(f"â° è¶…æ—¶ {attempt + 1}/{max_retries}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5 * (attempt + 1))
                    continue
                else:
                    raise Exception("Connection timeout after retries")
            
            except Exception as e:
                print(f"âŒ å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(3 * (attempt + 1))
                    continue
                else:
                    raise
        
        return papers

    async def _get_ndss_pdf_from_detail_page(self, session: aiohttp.ClientSession, detail_url: str) -> str:
        """ä» NDSS è®ºæ–‡è¯¦æƒ…é¡µè·å– PDF é“¾æ¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not detail_url:
            return ''
        
        try:
            # ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´æé«˜é€Ÿåº¦
            timeout = aiohttp.ClientTimeout(total=15, connect=5)
            
            async with session.get(detail_url, timeout=timeout) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # å°è¯•å¤šç§ PDF é“¾æ¥æ¨¡å¼
                    pdf_patterns = [
                        # ç›´æ¥ PDF é“¾æ¥
                        soup.find('a', href=re.compile(r'\.pdf$', re.I)),
                        # åŒ…å« "pdf" æ–‡æœ¬çš„é“¾æ¥
                        soup.find('a', string=re.compile(r'pdf', re.I)),
                        # åŒ…å« "download" çš„é“¾æ¥
                        soup.find('a', string=re.compile(r'download', re.I)),
                        # åœ¨ href ä¸­åŒ…å« "pdf" çš„é“¾æ¥
                        soup.find('a', href=re.compile(r'pdf', re.I))
                    ]
                    
                    for pdf_link in pdf_patterns:
                        if pdf_link and hasattr(pdf_link, 'get'):
                            pdf_url = pdf_link.get('href')
                            if pdf_url and isinstance(pdf_url, str):
                                # ç¡®ä¿ URL æ˜¯å®Œæ•´çš„
                                if not pdf_url.startswith('http'):
                                    if pdf_url.startswith('/'):
                                        pdf_url = f"https://www.ndss-symposium.org{pdf_url}"
                                    else:
                                        # ç›¸å¯¹è·¯å¾„
                                        base_url = '/'.join(detail_url.split('/')[:-1])
                                        pdf_url = f"{base_url}/{pdf_url}"
                                
                                # éªŒè¯ URL æ˜¯å¦ä»¥ .pdf ç»“å°¾æˆ–åŒ…å« pdf
                                if pdf_url.lower().endswith('.pdf') or 'pdf' in pdf_url.lower():
                                    return pdf_url
                    
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›´æ¥é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾å†…åµŒçš„PDF
                    iframe_pdf = soup.find('iframe', src=re.compile(r'\.pdf', re.I))
                    if iframe_pdf and hasattr(iframe_pdf, 'get'):
                        pdf_url = iframe_pdf.get('src')
                        if pdf_url and isinstance(pdf_url, str):
                            if not pdf_url.startswith('http'):
                                pdf_url = f"https://www.ndss-symposium.org{pdf_url}"
                            return pdf_url
                        
        except asyncio.TimeoutError:
            self.logger.debug(f"Timeout getting PDF from {detail_url}")
        except Exception as e:
            self.logger.debug(f"Error getting PDF from {detail_url}: {str(e)}")
            
        return ''

    async def _parse_sp_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è§£æSPè®ºæ–‡åˆ—è¡¨ - ä½¿ç”¨Computer.org GraphQL APIç›´æ¥è·å–proceedings ID"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        
        print(f"ğŸŒ æ­£åœ¨è·å– SP {year} proceedings ID...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Content-Type': 'application/json',
            'Origin': 'https://www.computer.org',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Dest': 'empty',
            'Referer': 'https://www.computer.org/csdl/proceedings/1000646',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache'
        }
        
        try:
            async with aiohttp.ClientSession(headers=headers) as session:
                # ä½¿ç”¨GraphQL APIè·å–SPä¼šè®®çš„æ‰€æœ‰proceedings
                graphql_url = "https://www.computer.org/csdl/api/v1/graphql"
                graphql_query = {
                    "variables": {"groupId": "1000646"},  # SPä¼šè®®çš„ç»„ID
                    "query": "query ($groupId: String) {\n  proceedings(groupId: $groupId) {\n    id\n    acronym\n    title\n    volume\n    displayVolume\n    year\n    __typename\n  }\n}"
                }
                
                timeout = aiohttp.ClientTimeout(total=60, connect=30)
                async with session.post(graphql_url, json=graphql_query, timeout=timeout) as response:
                    if response.status == 200:
                        data = await response.json()
                        proceedings_list = data.get('data', {}).get('proceedings', [])
                        
                        print(f"âœ… è·å–åˆ° {len(proceedings_list)} ä¸ª proceedings")
                        
                        # æŸ¥æ‰¾æŒ‡å®šå¹´ä»½çš„proceedings
                        target_proceeding = None
                        for proc in proceedings_list:
                            if str(proc.get('year')) == full_year:
                                target_proceeding = proc
                                break
                        
                        if not target_proceeding:
                            print(f"âŒ æœªæ‰¾åˆ° {full_year} å¹´çš„proceedings")
                            return papers
                        
                        proceedings_id = target_proceeding.get('id')
                        print(f"ğŸ†” æ‰¾åˆ° SP {full_year} proceedings ID: {proceedings_id}")
                        
                        # è°ƒç”¨Computer.org APIè·å–è®ºæ–‡æ•°æ®
                        all_papers = await self._call_computer_org_api(session, proceedings_id)
                        
                        # å¤„ç†æ‰€æœ‰è®ºæ–‡
                        if all_papers:
                            print(f"âœ… æˆåŠŸè·å– {len(all_papers)} ç¯‡è®ºæ–‡")
                            
                            # è§£æçœŸæ­£çš„PDFä¸‹è½½é“¾æ¥
                            print(f"ğŸ”— å¼€å§‹è§£æ {len(all_papers)} ä¸ªPDFä¸‹è½½é“¾æ¥...")
                            for i, paper in enumerate(all_papers):
                                if paper.get('needs_pdf_resolution') and paper.get('url'):
                                    print(f"ğŸ“‹ PDFè§£æè¿›åº¦: {i+1}/{len(all_papers)} - {paper.get('title', '')[:50]}...")
                                    
                                    real_pdf_url = await self._resolve_ieee_pdf_url(session, paper['url'])
                                    if real_pdf_url:
                                        paper['url'] = real_pdf_url
                                        print(f"âœ… è§£ææˆåŠŸ: {real_pdf_url[:60]}...")
                                    else:
                                        print(f"âŒ PDFé“¾æ¥è§£æå¤±è´¥")
                                    paper.pop('needs_pdf_resolution', None)
                            
                            papers = all_papers  # è¿”å›æ‰€æœ‰è®ºæ–‡
                        else:
                            papers = []
                        
                        return papers
                    else:
                        print(f"âŒ GraphQL APIè°ƒç”¨å¤±è´¥: HTTP {response.status}")
                        return []
                        
        except Exception as e:
            print(f"âŒ SPè§£æé”™è¯¯: {str(e)}")
            return []
    

    async def _call_computer_org_api(self, session: aiohttp.ClientSession, proceedings_id: str) -> List[Dict[str, Any]]:
        """è°ƒç”¨Computer.org APIè·å–è®ºæ–‡æ•°æ®"""
        papers = []
        api_url = f"https://www.computer.org/csdl/api/v1/citation/asciitext/proceedings/{proceedings_id}"
        
        print(f"ğŸ”— è°ƒç”¨Computer.org API: {api_url}")
        
        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„å®Œæ•´è¯·æ±‚å¤´
        api_headers = {
            'Host': 'www.computer.org',
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Referer': f'https://www.computer.org/csdl/proceedings/sp/2024/{proceedings_id}',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Priority': 'u=1, i',
            'Pragma': 'no-cache',
            'Cache-Control': 'no-cache'
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=60, connect=30)
            async with session.get(api_url, headers=api_headers, timeout=timeout) as response:
                if response.status == 200:
                    print(f"âœ… APIè°ƒç”¨æˆåŠŸ")
                    text_data = await response.text()
                    papers = self._parse_citation_data(text_data)
                    print(f"ğŸ“š è§£æåˆ° {len(papers)} ç¯‡è®ºæ–‡")
                    return papers
                else:
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥: HTTP {response.status}")
                    return []
                    
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨é”™è¯¯: {str(e)}")
            return []
    
    def _parse_citation_data(self, citation_text: str) -> List[Dict[str, Any]]:
        """è§£æå¼•ç”¨æ•°æ®è·å–è®ºæ–‡ä¿¡æ¯"""
        papers = []
        
        try:
            # æŒ‰æ¡ç›®åˆ†å‰²
            entries = re.split(r'\n\s*\n', citation_text.strip())
            
            for entry in entries:
                if not entry.strip():
                    continue
                
                paper_info = self._parse_single_citation(entry.strip())
                if paper_info:
                    papers.append(paper_info)
            
            return papers
            
        except Exception as e:
            self.logger.error(f"è§£æå¼•ç”¨æ•°æ®å¤±è´¥: {str(e)}")
            return []
    
    def _parse_single_citation(self, citation: str) -> Optional[Dict[str, Any]]:
        """è§£æå•ä¸ªå¼•ç”¨æ¡ç›®"""
        try:
            # æŸ¥æ‰¾æ ‡é¢˜ (é€šå¸¸åœ¨å¼•å·å†…æˆ–ä½œä¸ºç¬¬ä¸€è¡Œ)
            title_match = re.search(r'"([^"]+)"', citation)
            if not title_match:
                # å¤‡ç”¨ï¼šæå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                lines = citation.split('\n')
                title = lines[0].strip() if lines else ''
            else:
                title = title_match.group(1)
            
            if not title or len(title) < 10:
                return None
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«keywords:{}ï¼ˆç©ºå¤§æ‹¬å·ï¼‰ï¼Œè¿™è¡¨ç¤ºéè®ºæ–‡å†…å®¹
            if re.search(r'keywords:\s*\{\s*\}', citation, re.I):
                return None
            
            # è¿‡æ»¤ç‰¹å®šçš„éè®ºæ–‡æ ‡é¢˜æ¨¡å¼
            non_paper_patterns = [
                r'author\s+index',
                r'table\s+of\s+contents',
                r'program\s+committee',
                r'organiz(ing|ation)\s+committee',
                r'chair\s+message',
                r'welcome\s+message',
                r'foreword',
                r'preface',
                r'index\s+terms',
                r'subject\s+index'
            ]
            
            for pattern in non_paper_patterns:
                if re.search(pattern, title, re.I):
                    return None
            
            # æŸ¥æ‰¾ä½œè€…
            authors = []
            author_match = re.search(r'Author\(s\):\s*([^\n]+)', citation)
            if author_match:
                authors_text = author_match.group(1)
                authors = [author.strip() for author in authors_text.split(',')]
            
            # æŸ¥æ‰¾DOIé“¾æ¥ - ä¼˜å…ˆä½¿ç”¨æ ‡å‡†DOIæ ¼å¼
            doi_match = re.search(r'DOI:\s*(https?://[^\s]+)', citation)
            doi_url = doi_match.group(1) if doi_match else ''
            
            # å¦‚æœæ‰¾åˆ°çš„æ˜¯doi.ieeecomputersociety.orgé“¾æ¥ï¼Œè½¬æ¢ä¸ºæ ‡å‡†doi.orgæ ¼å¼
            if doi_url and 'doi.ieeecomputersociety.org' in doi_url:
                doi_url = doi_url.replace('doi.ieeecomputersociety.org', 'doi.org')
            
            # å¦‚æœæ²¡æœ‰æ ‡å‡†DOIï¼ŒæŸ¥æ‰¾æ‰€æœ‰URLä¸­åŒ…å«doi.ieeecomputersociety.orgçš„é“¾æ¥å¹¶è½¬æ¢
            if not doi_url:
                all_urls = re.findall(r'https?://[^\s]+', citation)
                for url in all_urls:
                    if 'doi.ieeecomputersociety.org' in url:
                        doi_url = url.replace('doi.ieeecomputersociety.org', 'doi.org')
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°åˆé€‚çš„é“¾æ¥ï¼Œè·³è¿‡è¿™ç¯‡è®ºæ–‡
            if not doi_url:
                return None
            
            return {
                'title': title,
                'authors': authors,
                'abstract': '',
                'url': doi_url,  # è¿™é‡Œå…ˆä¿å­˜DOIé“¾æ¥ï¼Œç¨åä¼šè§£æçœŸæ­£çš„PDFé“¾æ¥
                'doi': doi_url,
                'needs_pdf_resolution': True  # æ ‡è®°éœ€è¦è§£æPDFé“¾æ¥
            }
            
        except Exception as e:
            self.logger.error(f"è§£æå•ä¸ªå¼•ç”¨å¤±è´¥: {str(e)}")
            return None
    
    async def _resolve_ieee_pdf_url(self, session: aiohttp.ClientSession, doi_url: str) -> str:
        """ä»IEEE DOIé¡µé¢è§£æçœŸæ­£çš„PDFä¸‹è½½é“¾æ¥"""
        try:
            if not doi_url:
                return ''
            
            print(f"ğŸ” è§£æPDFé“¾æ¥: {doi_url[:50]}...")
            
            # è®¿é—®DOIé¡µé¢ï¼Œè·Ÿéšé‡å®šå‘
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            
            # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            async with session.get(doi_url, timeout=timeout, headers=headers, allow_redirects=True) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    print(f"âœ… æˆåŠŸè®¿é—®é¡µé¢: {str(response.url)[:60]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ°äº†Computer.orgé¡µé¢
                    current_url = str(response.url)
                    if 'computer.org' in current_url:
                        # åœ¨Computer.orgé¡µé¢æŸ¥æ‰¾PDFä¸‹è½½é“¾æ¥
                        return await self._extract_pdf_from_computer_org_page(soup, current_url)
                    else:
                        # åœ¨å…¶ä»–é¡µé¢ï¼ˆå¦‚doi.orgé‡å®šå‘é¡µï¼‰æŸ¥æ‰¾PDFé“¾æ¥
                        pdf_url = await self._extract_pdf_from_generic_page(soup, str(response.url))
                        # å¦‚æœæ˜¯IEEEé¡µé¢ï¼Œç›´æ¥è¿”å›æ„é€ çš„PDFé“¾æ¥
                        if 'ieeexplore.ieee.org' in current_url and not pdf_url:
                            match = re.search(r'/document/(\d+)', current_url)
                            if match:
                                arnumber = match.group(1)
                                pdf_url = f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={arnumber}"
                                print(f"âœ… æ„é€ PDFé“¾æ¥æˆåŠŸ: {pdf_url[:60]}...")
                        return pdf_url
                else:
                    print(f"âŒ DOIè®¿é—®å¤±è´¥: HTTP {response.status}")
                    
        except Exception as e:
            print(f"âŒ è§£æPDFé“¾æ¥å¤±è´¥: {str(e)}")
            
        return ''
    
    async def _extract_pdf_from_computer_org_page(self, soup, current_url: str) -> str:
        """ä»Computer.orgé¡µé¢æå–PDFé“¾æ¥"""
        # æŸ¥æ‰¾PDFä¸‹è½½é“¾æ¥çš„å¤šç§æ¨¡å¼
        pdf_patterns = [
            # Computer.orgç‰¹å®šçš„DOWNLOAD PDFæŒ‰é’®
            soup.find('a', string=re.compile(r'DOWNLOAD PDF', re.I)),
            soup.find('a', text=re.compile(r'download.*pdf', re.I)),
            soup.find('button', string=re.compile(r'download.*pdf', re.I)),
            
            # å¸¦æœ‰PDFç›¸å…³classçš„é“¾æ¥
            soup.find('a', class_=re.compile(r'download|pdf', re.I)),
            soup.find('a', attrs={'aria-label': re.compile(r'download|pdf', re.I)}),
            
            # ç›´æ¥PDFé“¾æ¥
            soup.find('a', href=re.compile(r'\.pdf$', re.I)),
            
            # Metaæ ‡ç­¾ä¸­çš„PDFé“¾æ¥
            soup.find('meta', attrs={'name': 'citation_pdf_url'}),
            soup.find('meta', attrs={'property': 'citation_pdf_url'})
        ]
        
        for pattern in pdf_patterns:
            if pattern:
                if pattern.name == 'meta':
                    pdf_url = pattern.get('content')
                else:
                    pdf_url = pattern.get('href')
                    
                if pdf_url:
                    # è¡¥å…¨ç›¸å¯¹URL
                    if not pdf_url.startswith('http'):
                        if pdf_url.startswith('/'):
                            pdf_url = f"https://www.computer.org{pdf_url}"
                        else:
                            base_url = '/'.join(current_url.split('/')[:-1])
                            pdf_url = f"{base_url}/{pdf_url}"
                    
                    print(f"âœ… æ‰¾åˆ°PDFé“¾æ¥: {pdf_url[:60]}...")
                    return pdf_url
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç›´æ¥é“¾æ¥ï¼Œå°è¯•æŸ¥æ‰¾data-*å±æ€§ä¸­PDFé“¾æ¥
        for element in soup.find_all(attrs={'data-pdf-url': True}):
            pdf_url = element.get('data-pdf-url')
            if pdf_url:
                if not pdf_url.startswith('http'):
                    pdf_url = f"https://www.computer.org{pdf_url}"
                print(f"ğŸ”— ä»dataå±æ€§æ‰¾åˆ°PDF: {pdf_url[:60]}...")
                return pdf_url
        
        # å°è¯•æŸ¥æ‰¾åŒ…å«PDFçš„æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href')
            if href and ('pdf' in href.lower() or 'download' in href.lower()):
                if not href.startswith('http'):
                    if href.startswith('/'):
                        href = f"https://www.computer.org{href}"
                    else:
                        base_url = '/'.join(current_url.split('/')[:-1])
                        href = f"{base_url}/{href}"
                print(f"ğŸ” å€™é€‰PDFé“¾æ¥: {href[:60]}...")
                return href
        
        # æœ€åå°è¯•ï¼šä»å½“å‰é¡µé¢URLæ„é€ PDFé“¾æ¥
        if '/proceedings-article/' in current_url:
            # æå–æ–‡ç« ID
            parts = current_url.rstrip('/').split('/')
            if parts:
                article_id = parts[-1]
                # ä½¿ç”¨çœŸæ­£çš„Computer.org PDFä¸‹è½½API
                pdf_url = f"https://www.computer.org/csdl/pds/api/csdl/proceedings/download-article/{article_id}/pdf"
                print(f"ğŸ”— æ„é€ PDF APIé“¾æ¥: {pdf_url}")
                return pdf_url
        
        return ''
    

    
    

    async def _parse_usenix_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è§£æUSENIX Securityè®ºæ–‡åˆ—è¡¨ - ç²¾ç¡®ä¿®å¤ç‰ˆæœ¬"""
        papers = []
        url = f"https://www.usenix.org/conference/usenixsecurity{year}/technical-sessions"
        
        print(f"ğŸŒ æ­£åœ¨è§£æ USENIX Security {year} è®ºæ–‡åˆ—è¡¨...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            async with aiohttp.ClientSession(headers=headers) as session:
                timeout = aiohttp.ClientTimeout(total=30, connect=10)
                
                async with session.get(url, timeout=timeout) as response:
                    if response.status == 200:
                        print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸï¼Œå¼€å§‹è§£æ...")
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨æ‰¾åˆ°è®ºæ–‡èŠ‚ç‚¹
                        paper_nodes = soup.find_all('article', class_='node node-paper view-mode-schedule')
                        
                        print(f"ğŸ“š æ‰¾åˆ° {len(paper_nodes)} ä¸ªè®ºæ–‡èŠ‚ç‚¹")
                        
                        if not paper_nodes:
                            print(f"âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡èŠ‚ç‚¹")
                            return papers
                        
                        for idx, node in enumerate(paper_nodes):
                            # æ˜¾ç¤ºè¿›åº¦
                            if idx % 10 == 0 or idx == len(paper_nodes) - 1:
                                progress = (idx + 1) / len(paper_nodes) * 100
                                print(f"ğŸ” è§£æè¿›åº¦: {progress:.1f}% ({idx+1}/{len(paper_nodes)})")
                            
                            try:
                                # æå–æ ‡é¢˜ - ä½¿ç”¨h2æ ‡ç­¾
                                title_elem = node.find('h2')
                                if not title_elem:
                                    continue
                                
                                # ä»é“¾æ¥ä¸­è·å–æ ‡é¢˜æ–‡æœ¬
                                link_elem = title_elem.find('a')
                                if not link_elem:
                                    continue
                                    
                                title = link_elem.get_text().strip()
                                
                                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆè®ºæ–‡æ ‡é¢˜
                                if not self._is_valid_paper_title(title):
                                    continue
                                
                                # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                                print(f"ğŸ“„ [{idx+1}/{len(paper_nodes)}] æ‰¾åˆ°è®ºæ–‡: {title[:70]}{'...' if len(title) > 70 else ''}")
                                
                                # æå–ä½œè€…ä¿¡æ¯
                                authors = []
                                author_container = node.find('div', class_='field-name-field-paper-people-text')
                                if author_container:
                                    authors_text = author_container.get_text().strip()
                                    if authors_text:
                                        # ç®€å•è§£æä½œè€…åˆ—è¡¨
                                        authors = [authors_text.split(',')[0].strip()] if ',' in authors_text else [authors_text]
                                
                                # è·å–PDFé“¾æ¥
                                pdf_url = await self._get_usenix_pdf_url_simple(session, node)
                                
                                if pdf_url:
                                    papers.append({
                                        'title': title,
                                        'authors': authors,
                                        'abstract': '',
                                        'url': pdf_url,
                                        'doi': ''
                                    })
                                    
                            except Exception as e:
                                self.logger.debug(f"è§£æèŠ‚ç‚¹ {idx} å¤±è´¥: {str(e)}")
                                continue
                        
                        print(f"âœ… USENIX è§£æå®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
                        return papers
                    
                    elif response.status == 404:
                        print(f"âŒ USENIX {year} é¡µé¢ä¸å­˜åœ¨")
                        return []
                    else:
                        print(f"âŒ HTTP {response.status}")
                        return []
                        
        except asyncio.TimeoutError:
            print(f"â° è®¿é—®è¶…æ—¶")
            return []
        except Exception as e:
            print(f"âŒ è§£æé”™è¯¯: {str(e)}")
            return []

    async def _get_usenix_pdf_url_simple(self, session: aiohttp.ClientSession, node) -> str:
        """ç®€åŒ–çš„USENIX PDFé“¾æ¥è·å–æ–¹æ³•"""
        try:
            # 1. é¦–å…ˆæŸ¥æ‰¾ç›´æ¥çš„PDFé“¾æ¥
            pdf_link = node.find('a', href=re.compile(r'\.pdf$', re.I))
            if pdf_link:
                href = pdf_link.get('href')
                if href:
                    return self._complete_usenix_url(href)
            
            # 2. æŸ¥æ‰¾presentationé¡µé¢é“¾æ¥
            presentation_link = node.find('a', href=re.compile(r'/presentation/', re.I))
            if presentation_link:
                presentation_url = presentation_link.get('href')
                if presentation_url:
                    presentation_url = self._complete_usenix_url(presentation_url)
                    
                    # ä»presentationé¡µé¢è·å–PDFé“¾æ¥
                    try:
                        timeout = aiohttp.ClientTimeout(total=10, connect=5)
                        async with session.get(presentation_url, timeout=timeout) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'html.parser')
                                
                                # æŸ¥æ‰¾PDFä¸‹è½½é“¾æ¥
                                pdf_link = soup.find('a', href=re.compile(r'\.pdf$', re.I))
                                if pdf_link:
                                    pdf_url = pdf_link.get('href')
                                    if pdf_url:
                                        return self._complete_usenix_url(pdf_url)
                    except Exception as e:
                        self.logger.debug(f"presentationé¡µé¢è·å–å¤±è´¥: {str(e)}")
            
            return ''
            
        except Exception as e:
            self.logger.debug(f"è·å–PDFé“¾æ¥å¤±è´¥: {str(e)}")
            return ''
    
    def _is_valid_paper_title(self, title: str) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è®ºæ–‡æ ‡é¢˜"""
        if not title or len(title) < 10:  # é™ä½æœ€å°é•¿åº¦è¦æ±‚
            return False
        
        # åªæ’é™¤æ˜æ˜¾çš„éè®ºæ–‡å†…å®¹ï¼Œå‡å°‘è¿‡æ»¤
        exclude_keywords = [
            'technical session', 'session chair', 'keynote', 'tutorial', 
            'workshop', 'break', 'lunch', 'coffee break', 'opening remarks',
            'closing remarks', 'panel discussion', 'poster session'
        ]
        
        title_lower = title.lower()
        for keyword in exclude_keywords:
            if keyword in title_lower:
                return False
        
        # æ”¾å®½æ¡ä»¶ï¼Œåªè¦æœ‰ä¸€å®šé•¿åº¦å°±è®¤ä¸ºæ˜¯æœ‰æ•ˆè®ºæ–‡
        return len(title) > 15
    
    def _complete_usenix_url(self, url: str) -> str:
        """è¡¥å…¨USENIX URL"""
        if not url:
            return ''
        
        if url.startswith('http'):
            return url
        elif url.startswith('/'):
            return f"https://www.usenix.org{url}"
        else:
            return f"https://www.usenix.org/{url}"


    def _extract_ieee_pdf_url(self, paper_element) -> str:
        """ä»IEEEé¡µé¢å…ƒç´ ä¸­æå–PDFé“¾æ¥"""
        try:
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_link = paper_element.find('a', href=re.compile(r'.*\.pdf'))
            if pdf_link:
                return pdf_link['href']
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥å¹¶æ„é€ PDF URL
            article_link = paper_element.find('a', href=re.compile(r'/document/'))
            if article_link:
                doc_id = re.search(r'/document/(\d+)', article_link['href'])
                if doc_id:
                    return f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_id.group(1)}"
            
            return ''
        except Exception as e:
            self.logger.error(f"Error extracting IEEE PDF URL: {str(e)}")
            return ''
        """ä»IEEEé¡µé¢å…ƒç´ ä¸­æå–PDFé“¾æ¥"""
        try:
            # æŸ¥æ‰¾PDFé“¾æ¥
            pdf_link = paper_element.find('a', href=re.compile(r'.*\.pdf'))
            if pdf_link:
                return pdf_link['href']
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥å¹¶æ„é€ PDF URL
            article_link = paper_element.find('a', href=re.compile(r'/document/'))
            if article_link:
                doc_id = re.search(r'/document/(\d+)', article_link['href'])
                if doc_id:
                    return f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_id.group(1)}"
            
            return ''
        except Exception as e:
            self.logger.error(f"Error extracting IEEE PDF URL: {str(e)}")
            return ''




    async def _download_with_retry(self, url: str) -> Optional[bytes]:
        """å¸¦é‡è¯•æœºåˆ¶çš„ä¸‹è½½ - ä¿®å¤ç‰ˆæœ¬ï¼Œæ”¯æŒIEEE SPè®ºæ–‡ç‰¹æ®Šæµç¨‹"""
        if not url:
            return None

        # å¯¹Computer.orgçš„SPè®ºæ–‡ä½¿ç”¨ç‰¹æ®Šå¤„ç†
        if 'computer.org/csdl/pds/api' in url:
            return await self._download_computer_org_pdf(url)

        # å¯¹IEEE SPè®ºæ–‡ç‰¹æ®Šå¤„ç†
        if 'ieeexplore.ieee.org/stampPDF/getPDF.jsp' in url:
            return await self._download_ieee_pdf_with_httpx(url)

        # å…¶ä»–é“¾æ¥ä½¿ç”¨åŸæœ‰æ–¹æ³•
        return await self._download_with_aiohttp(url)

    async def _download_ieee_pdf_with_httpx(self, url: str) -> Optional[bytes]:
        """ä½¿ç”¨httpx+HTTP2è‡ªåŠ¨è·å–ERIGHTSå¹¶ä¸‹è½½IEEE SP PDF"""
        try:
            import httpx
            # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ŒERIGHTS=0000
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Dnt': '1',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Dest': 'iframe',
                'Referer': 'https://ieeexplore.ieee.org/',
                'Sec-Ch-Ua': '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Priority': 'u=0, i',
            }
            cookies = {'ERIGHTS': '0000'}
            timeout = 60
            async with httpx.AsyncClient(http2=True, timeout=timeout, follow_redirects=False) as client:
                # ç¬¬ä¸€æ¬¡è¯·æ±‚
                resp1 = await client.get(url, headers=headers, cookies=cookies)
                # æ£€æŸ¥302å’ŒSet-Cookie
                if resp1.status_code in (302, 303, 307, 301):
                    set_cookie = resp1.headers.get('set-cookie', '')
                    # æå–ERIGHTS
                    import re
                    m = re.search(r'ERIGHTS=([^;]+)', set_cookie)
                    if m:
                        erights_val = m.group(1)
                        cookies['ERIGHTS'] = erights_val
                        # è·ŸéšLocation
                        next_url = resp1.headers.get('location', url)
                        # ç¬¬äºŒæ¬¡è¯·æ±‚
                        resp2 = await client.get(url, headers=headers, cookies=cookies)
                        if resp2.status_code == 200 and resp2.content and resp2.content[:4] == b'%PDF':
                            return resp2.content
                        # æœ‰æ—¶éœ€è¦å†è¯·æ±‚ä¸€æ¬¡
                        if resp2.status_code in (302, 303, 307, 301):
                            # å†æ¬¡å°è¯•
                            resp3 = await client.get(url, headers=headers, cookies=cookies)
                            if resp3.status_code == 200 and resp3.content and resp3.content[:4] == b'%PDF':
                                return resp3.content
                    else:
                        # æ²¡æœ‰ERIGHTSï¼Œç›´æ¥å°è¯•å†…å®¹
                        if resp1.status_code == 200 and resp1.content and resp1.content[:4] == b'%PDF':
                            return resp1.content
                elif resp1.status_code == 200 and resp1.content and resp1.content[:4] == b'%PDF':
                    return resp1.content
        except Exception as e:
            self.logger.error(f"httpxä¸‹è½½IEEE PDFå¤±è´¥: {str(e)}")
        return None
    
    async def _download_computer_org_pdf(self, api_url: str) -> Optional[bytes]:
        """ä¸“é—¨ä¸ºComputer.org PDFä¸‹è½½çš„æ–¹æ³• - ä½¿ç”¨curl"""
        return await self._download_with_curl(api_url)
    
    async def _download_with_curl(self, url: str) -> Optional[bytes]:
        """ä½¿ç”¨curlå‘½ä»¤ä¸‹è½½"""
        try:
            import asyncio
            import subprocess
            
            cmd = [
                'curl',
                '-L',  # è·Ÿéšé‡å®šå‘
                '-s',  # é™é»˜æ¨¡å¼
                '--max-time', '60',  # æœ€å¤§è¶…æ—¶æ—¶é—´
                '--user-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36',
                '--header', 'Accept: application/pdf,application/octet-stream,*/*',
                url
            ]
            
            # ä½¿ç”¨subprocesså¼‚æ­¥æ‰§è¡Œcurl
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0 and stdout and len(stdout) > 1024:
                # éªŒè¯PDFæ–‡ä»¶ç±»å‹
                if self._is_valid_pdf(stdout):
                    return stdout
                else:
                    return None
            else:
                error_msg = stderr.decode('utf-8', errors='ignore') if stderr else 'Unknown error'
                
        except Exception as e:
            pass
            
        return None

    async def _download_with_aiohttp(self, url: str) -> Optional[bytes]:
        """åŸæœ‰çš„aiohttpä¸‹è½½æ–¹æ³•"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/pdf,application/octet-stream,*/*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        }
        
        for attempt in range(self.max_retries):
            try:
                # ä¸ºæ¯æ¬¡é‡è¯•åˆ›å»ºæ–°çš„Sessionï¼Œé¿å…è¿æ¥é—®é¢˜
                connector = aiohttp.TCPConnector(
                    limit=5,
                    limit_per_host=2,
                    force_close=True,  # å¼ºåˆ¶å…³é—­è¿æ¥é¿å…å¤ç”¨é—®é¢˜
                    enable_cleanup_closed=True
                )
                
                timeout = aiohttp.ClientTimeout(
                    total=90,
                    connect=30,
                    sock_read=60
                )
                
                async with aiohttp.ClientSession(
                    headers=headers,
                    timeout=timeout,
                    connector=connector
                ) as session:
                    async with session.get(url) as response:
                        if response.status == 200:
                            content = await response.read()
                            if len(content) > 0:
                                return content
                            else:
                                self.logger.warning(f"Empty content from {url}")
                        elif response.status in [429, 503, 502]:
                            self.logger.warning(
                                f"Server busy (attempt {attempt + 1}/{self.max_retries}): "
                                f"HTTP {response.status} for {url}"
                            )
                            await asyncio.sleep(self.retry_delay * (attempt + 1) * 2)
                            continue
                        else:
                            self.logger.warning(
                                f"Download failed (attempt {attempt + 1}/{self.max_retries}): "
                                f"HTTP {response.status} for {url}"
                            )

            except asyncio.TimeoutError:
                self.logger.warning(
                    f"Download timeout (attempt {attempt + 1}/{self.max_retries}): {url}"
                )
            except Exception as e:
                self.logger.warning(
                    f"Download error (attempt {attempt + 1}/{self.max_retries}): {str(e)} for {url}"
                )

            # é‡è¯•é—´éš”
            if attempt < self.max_retries - 1:
                delay = self.retry_delay * (attempt + 1)
                await asyncio.sleep(delay)

        self.logger.error(f"Failed to download after {self.max_retries} attempts: {url}")
        return None

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶å"""
        # ç§»é™¤ä¸å…è®¸çš„å­—ç¬¦
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '')

        # å°†ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        filename = filename.replace(' ', '_')

        # é™åˆ¶é•¿åº¦
        max_length = 255 - len('.pdf')
        if len(filename) > max_length:
            filename = filename[:max_length]

        return filename.strip('._')
    
    def _is_valid_pdf(self, content: bytes) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„PDFæ–‡ä»¶"""
        if not content or len(content) < 4:
            return False
        
        # æ£€æŸ¥PDFæ–‡ä»¶å¤´ï¼ˆ%PDF-ï¼‰
        return content.startswith(b'%PDF-')
    

