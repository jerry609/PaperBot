# securipaperbot/utils/downloader.py

from typing import Dict, List, Any, Optional
import aiohttp
import asyncio
import httpx
from pathlib import Path
import urllib.parse
from bs4 import BeautifulSoup
import re
import json
import time
import random
from datetime import datetime
import logging
import traceback

# æ·»åŠ åŠ¨æ€cookieè·å–æ”¯æŒ
import traceback
try:
    # curl_cffi 0.5.x ç‰ˆæœ¬ä¸­, AsyncSession ä½äº requests æ¨¡å—ä¸‹
    from curl_cffi.requests import AsyncSession
    CURL_CFFI_AVAILABLE = True
except ImportError:
    from typing import Any as AsyncSession # Mock for type hinting
    CURL_CFFI_AVAILABLE = False
    print("âŒ 'curl_cffi' å¯¼å…¥å¤±è´¥ã€‚è¯¦ç»†é”™è¯¯ä¿¡æ¯å¦‚ä¸‹:")
    traceback.print_exc()
    print("è­¦å‘Š: curl_cffi æœªå®‰è£…æˆ–æ— æ³•åŠ è½½ï¼ŒåŠ¨æ€cookieè·å–åŠŸèƒ½(å¦‚ACM)å°†å—é™")

try:
    import cloudscraper
    CLOUDSCRAPER_AVAILABLE = True
except ImportError:
    CLOUDSCRAPER_AVAILABLE = False
    print("è­¦å‘Š: cloudscraper æœªå®‰è£…ï¼ŒåŠ¨æ€cookieè·å–åŠŸèƒ½(å¦‚ACM)å°†å—é™")


# ä½¿ç”¨æ ‡å‡†æ—¥å¿—ï¼Œé¿å…ç›¸å¯¹å¯¼å…¥é—®é¢˜
def setup_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å·¥å…·ç±» - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æŒä¹…åŒ–ä¼šè¯"""
    
    # ä¼šè®®åŸºæœ¬ä¿¡æ¯é…ç½®
    CONFERENCE_INFO = {
        'sp': {
            'base_url': 'https://ieeexplore.ieee.org/xpl/conhome/1000487/all-proceedings',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'
            }
        },
        'ndss': {
            'base_url': 'https://www.ndss-symposium.org',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'
            }
        },
        'usenix': {
            'base_url': 'https://www.usenix.org/conference/usenixsecurity',
            'headers': {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/116.0'
            }
        }
    }
    
    async def _download_with_retry(self, url: str) -> Optional[bytes]:
        """
        æ™ºèƒ½ä¸‹è½½å®ç°ï¼Œå¸¦è‡ªåŠ¨é‡è¯•å’Œåçˆ¬å¤„ç†ã€‚
        
        Args:
            url (str): è¦ä¸‹è½½çš„URL
            
        Returns:
            Optional[bytes]: ä¸‹è½½çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        # éªŒè¯å¹¶ç¡®ä¿ä¼šè¯å¯ç”¨
        if not self.session:
            try:
                self.logger.info("æ­£åœ¨é‡æ–°åˆ›å»ºæŒä¹…åŒ–ä¼šè¯...")
                self.session = AsyncSession()
            except Exception as e:
                self.logger.error(f"åˆ›å»ºæŒä¹…åŒ–ä¼šè¯å¤±è´¥: {e}")
                return None

        last_error = None
        content = None
        
        for attempt in range(1, self.max_retries + 1):
            try:
                self.logger.info(f"ä¸‹è½½å°è¯• {attempt}/{self.max_retries}: {url}")
                
                # é…ç½®ç‰¹æ®Šheadersä»¥ç»•è¿‡åçˆ¬
                headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                # éªŒè¯ä¼šè¯çŠ¶æ€
                if not self.session or getattr(self.session, '_closed', False):
                    self.logger.warning("ä¼šè¯å·²å…³é—­ï¼Œæ­£åœ¨é‡æ–°åˆ›å»º...")
                    self.session = AsyncSession()
                
                # ä½¿ç”¨curl_cffiçš„æŒä¹…åŒ–ä¼šè¯å’Œæµè§ˆå™¨ä»¿çœŸ
                response = await self.session.get(
                    url,
                    impersonate="chrome110",
                    headers=headers,
                    timeout=60
                )
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç 
                if response.status_code == 403:
                    self.logger.warning(f"é‡åˆ°403 Forbiddenï¼Œå¯èƒ½æ˜¯åçˆ¬é™åˆ¶ (å°è¯• {attempt}/{self.max_retries})")
                    await asyncio.sleep(self.retry_delay * attempt)  # æŒ‡æ•°é€€é¿
                    continue
                    
                elif response.status_code == 429:
                    self.logger.warning(f"é‡åˆ°429 Too Many Requestsï¼Œå¼€å§‹ç­‰å¾… (å°è¯• {attempt}/{self.max_retries})")
                    await asyncio.sleep(self.retry_delay * 2 * attempt)  # æ›´é•¿çš„ç­‰å¾…
                    continue
                    
                elif response.status_code != 200:
                    self.logger.warning(f"HTTP {response.status_code} (å°è¯• {attempt}/{self.max_retries})")
                    await asyncio.sleep(self.retry_delay)
                    continue
                
                # è·å–å“åº”å†…å®¹
                content = response.content
                
                # éªŒè¯å†…å®¹
                if not content or len(content) < 1024:  # å°äº1KBå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                    self.logger.warning(f"å“åº”å†…å®¹è¿‡å°: {len(content) if content else 0} bytes")
                    continue
                
                # å¯¹äºPDFï¼ŒéªŒè¯æ–‡ä»¶å¤´
                if url.lower().endswith('.pdf') and not content.startswith(b'%PDF'):
                    self.logger.warning("å“åº”ä¸æ˜¯æœ‰æ•ˆçš„PDFæ ¼å¼")
                    continue
                    
                self.logger.info(f"âœ… æˆåŠŸä¸‹è½½: {len(content)} bytes")
                return content
                
            except Exception as e:
                last_error = e
                self.logger.warning(f"ä¸‹è½½å‡ºé”™ (å°è¯• {attempt}/{self.max_retries}): {e}")
                await asyncio.sleep(self.retry_delay)
                continue
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        if last_error:
            self.logger.error(f"âŒ ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚æœ€åé”™è¯¯: {last_error}")
        else:
            self.logger.error("âŒ ä¸‹è½½å¤±è´¥ï¼Œæœªè·å¾—æœ‰æ•ˆå†…å®¹")
        return None

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†å¹¶è§„èŒƒåŒ–æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # æ›¿æ¢ Windows æ–‡ä»¶ç³»ç»Ÿä¸å…è®¸çš„å­—ç¬¦
        invalid_chars = r'[\\/:"*?<>|]+'
        filename = re.sub(invalid_chars, '_', filename)
        
        # å°†è¿ç»­çš„ç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        filename = re.sub(r'\s+', ' ', filename)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        filename = filename.strip()
        
        # å¦‚æœæ–‡ä»¶åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not filename:
            filename = f"paper_{int(time.time())}"
            
        # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆWindows æœ€å¤§è·¯å¾„é•¿åº¦ä¸º 260 å­—ç¬¦ï¼‰
        max_length = 200  # ç•™ä¸€äº›ä½™åœ°ç»™è·¯å¾„å’Œæ‰©å±•å
        if len(filename) > max_length:
            filename = filename[:max_length-3] + "..."
            
        return filename

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = setup_logger(__name__)
        self.download_path = Path(self.config.get('download_path', './papers'))
        self.download_path.mkdir(parents=True, exist_ok=True)
        
        self.session: Optional[AsyncSession] = None

        # é…ç½®ä¸‹è½½é‡è¯•å‚æ•°
        self.max_retries = self.config.get('max_retries', 3)
        self.retry_delay = self.config.get('retry_delay', 3)

        # å¹¶å‘æ§åˆ¶
        max_concurrent = 1
        self.semaphore = asyncio.Semaphore(max_concurrent)


        # ä¼šè®®URLæ¨¡æ¿
        self.conference_urls = {
            'ccs': 'https://dl.acm.org/doi/proceedings/',
            'sp': 'https://ieeexplore.ieee.org/xpl/conhome/',
            'ndss': 'https://www.ndss-symposium.org/',
            'usenix': 'https://www.usenix.org/conference/'
        }

    async def __aenter__(self):
        """åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæŒä¹…åŒ–çš„ curl_cffi ä¼šè¯."""
        try:
            if self.session and not getattr(self.session, '_closed', False):
                self.logger.info("ä½¿ç”¨ç°æœ‰çš„æŒä¹…åŒ–ä¼šè¯...")
                return self
                
            self.logger.info("æ­£åœ¨åˆ›å»ºæ–°çš„æŒä¹…åŒ–ä¼šè¯...")
            self.session = AsyncSession()
            return self
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæŒä¹…åŒ–ä¼šè¯å¤±è´¥: {e}")
            raise

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å…³é—­æŒä¹…åŒ–ä¼šè¯."""
        try:
            if self.session:
                # æ£€æŸ¥ä¼šè¯æ˜¯å¦å·²ç»å…³é—­
                is_closed = getattr(self.session, '_closed', True)
                
                if not is_closed and hasattr(self.session, 'close'):
                    try:
                        self.logger.info("æ­£åœ¨å…³é—­æŒä¹…åŒ–ä¼šè¯...")
                        await self.session.close()
                    except Exception as e:
                        self.logger.warning(f"å…³é—­ä¼šè¯æ—¶å‡ºç°å¼‚å¸¸: {e}")
                else:
                    self.logger.info("ä¼šè¯å·²ç»å…³é—­ï¼Œæ— éœ€å†æ¬¡å…³é—­")
        except Exception as e:
            self.logger.warning(f"å¤„ç†ä¼šè¯å…³é—­æ—¶å‡ºç°å¼‚å¸¸: {e}")
        finally:
            # ç¡®ä¿ä¼šè¯å¯¹è±¡è¢«æ¸…ç†
            self.session = None

    async def download_paper(self, url: str, title: str, paper_index: int = 0, total_papers: int = 0) -> Dict[str, Any]:
        """ä¸‹è½½å•ç¯‡è®ºæ–‡ - ä¼˜åŒ–ç‰ˆæœ¬"""
        async with self.semaphore:
            try:
                # ç”Ÿæˆæ–‡ä»¶å - ä¸ºIEEEè®ºæ–‡æ·»åŠ ç‰¹æ®Šå‰ç¼€
                safe_title = self._sanitize_filename(title)
                
                # ä½¿ç”¨ç®€åŒ–çš„æ–‡ä»¶åï¼šåªä½¿ç”¨è®ºæ–‡æ ‡é¢˜
                filename = f"{safe_title}.pdf"
                
                file_path = self.download_path / filename

                # æ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼ˆä¸NDSS/USENIXä¿æŒä¸€è‡´ï¼‰
                if total_papers > 0:
                    progress = (paper_index + 1) / total_papers * 100
                    print(f"ğŸ’¾ [{paper_index+1}/{total_papers}] ä¸‹è½½: {title[:50]}{'...' if len(title) > 50 else ''}")

                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½å¹¶éªŒè¯æ–‡ä»¶
                if file_path.exists():
                    # éªŒè¯æ–‡ä»¶å¤§å°ï¼Œè¿‡å°çš„æ–‡ä»¶å¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                    file_size = file_path.stat().st_size
                    if file_size > 1024:  # å¤§äº1KBè®¤ä¸ºæœ‰æ•ˆ
                        return {
                            'success': True,
                            'path': str(file_path),
                            'cached': True,
                            'size': file_size
                        }
                    else:
                        # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                        file_path.unlink()
                        self.logger.warning(f"Removed invalid cached file: {file_path}")

                # ä¸‹è½½è®ºæ–‡
                content = await self._download_with_retry(url)
                if content:
                    # éªŒè¯ä¸‹è½½å†…å®¹
                    if len(content) < 1024:
                        raise Exception(f"Downloaded content too small ({len(content)} bytes), likely an error page")
                    
                    # ä¿å­˜æ–‡ä»¶
                    file_path.write_bytes(content)
                    file_size = len(content)

                    return {
                        'success': True,
                        'path': str(file_path),
                        'cached': False,
                        'size': file_size
                    }
                else:
                    raise Exception("Failed to download paper - no content received")

            except Exception as e:
                self.logger.error(f"Error downloading paper {title}: {str(e)}")
                return {
                    'success': False,
                    'error': str(e)
                }

    async def _parse_sp_papers(self, year: str) -> List[Dict[str, Any]]:
        """è§£æ IEEE S&P è®ºæ–‡åˆ—è¡¨"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        
        try:
            print(f"ğŸ“š æ­£åœ¨è·å– IEEE S&P {full_year} è®ºæ–‡åˆ—è¡¨...")
            conf_info = self.CONFERENCE_INFO['sp']
            base_url = f"{conf_info['base_url']}"
            
            # ä½¿ç”¨ä¼šè¯å‘é€è¯·æ±‚
            if not self.session:
                self.session = AsyncSession()
                
            response = await self.session.get(
                base_url,
                headers=conf_info['headers'],
                impersonate="chrome110"
            )
            
            if response.status_code != 200:
                raise Exception(f"è·å–ä¼šè®®é¡µé¢å¤±è´¥: HTTP {response.status_code}")
                
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            paper_items = soup.select('div.paper-item')
            
            for item in paper_items:
                title_elem = item.select_one('h3.paper-title')
                if not title_elem:
                    continue
                    
                title = title_elem.text.strip()
                url = item.select_one('a[href*=".pdf"]')
                if not url:
                    continue
                    
                pdf_url = url['href']
                if not pdf_url.startswith('http'):
                    pdf_url = f"https://ieeexplore.ieee.org{pdf_url}"
                    
                papers.append({
                    'title': title,
                    'url': pdf_url
                })
                
            print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"âŒ è·å– IEEE S&P {full_year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    async def _parse_ndss_papers(self, year: str) -> List[Dict[str, Any]]:
        """è§£æ NDSS è®ºæ–‡åˆ—è¡¨"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        
        try:
            print(f"ğŸ“š æ­£åœ¨è·å– NDSS {full_year} è®ºæ–‡åˆ—è¡¨...")
            conf_info = self.CONFERENCE_INFO['ndss']
            base_url = f"{conf_info['base_url']}/ndss{year}/accepted-papers"
            
            # ä½¿ç”¨ä¼šè¯å‘é€è¯·æ±‚
            if not self.session:
                self.session = AsyncSession()
                
            response = await self.session.get(
                base_url,
                headers=conf_info['headers'],
                impersonate="chrome110"
            )
            
            if response.status_code != 200:
                raise Exception(f"è·å–ä¼šè®®é¡µé¢å¤±è´¥: HTTP {response.status_code}")
                
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            paper_items = soup.select('div.paper-item, div.accepted-paper')
            
            for item in paper_items:
                title_elem = item.select_one('h3.paper-title, h4.paper-title, div.paper-title')
                if not title_elem:
                    continue
                    
                title = title_elem.text.strip()
                url = item.select_one('a[href*=".pdf"]')
                if not url:
                    continue
                    
                pdf_url = url['href']
                if not pdf_url.startswith('http'):
                    pdf_url = f"{conf_info['base_url']}{pdf_url}"
                    
                papers.append({
                    'title': title,
                    'url': pdf_url
                })
                
            print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"âŒ è·å– NDSS {full_year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    async def _parse_usenix_papers(self, year: str) -> List[Dict[str, Any]]:
        """è§£æ USENIX Security è®ºæ–‡åˆ—è¡¨"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        
        try:
            print(f"ğŸ“š æ­£åœ¨è·å– USENIX Security {full_year} è®ºæ–‡åˆ—è¡¨...")
            conf_info = self.CONFERENCE_INFO['usenix']
            base_url = f"{conf_info['base_url']}{full_year}/technical-sessions"
            
            # ä½¿ç”¨ä¼šè¯å‘é€è¯·æ±‚
            if not self.session:
                self.session = AsyncSession()
                
            response = await self.session.get(
                base_url,
                headers=conf_info['headers'],
                impersonate="chrome110"
            )
            
            if response.status_code != 200:
                raise Exception(f"è·å–ä¼šè®®é¡µé¢å¤±è´¥: HTTP {response.status_code}")
                
            # è§£æé¡µé¢å†…å®¹
            soup = BeautifulSoup(response.text, 'html.parser')
            paper_items = soup.select('div.paper-item, div.node-paper')
            
            for item in paper_items:
                title_elem = item.select_one('h2.node-title, div.field-title')
                if not title_elem:
                    continue
                    
                title = title_elem.text.strip()
                url = item.select_one('a[href*=".pdf"]')
                if not url:
                    continue
                    
                pdf_url = url['href']
                if not pdf_url.startswith('http'):
                    pdf_url = f"https://www.usenix.org{pdf_url}"
                    
                papers.append({
                    'title': title,
                    'url': pdf_url
                })
                
            print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            print(f"âŒ è·å– USENIX Security {full_year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    async def get_conference_papers(self, conference: str, year: str) -> List[Dict[str, Any]]:
        """è·å–ä¼šè®®è®ºæ–‡åˆ—è¡¨ - å¸¦è¿›åº¦æ˜¾ç¤º"""
        try:
            conf_info = self.CONFERENCE_INFO.get(conference)
            if not conf_info and conference != 'ccs':
                raise ValueError(f"ä¸æ”¯æŒçš„ä¼šè®®: {conference}")

            papers = []
            print(f"ğŸ” æ­£åœ¨è·å– {conference.upper()} {year} è®ºæ–‡åˆ—è¡¨...")

            # æ ¹æ®ä¼šè®®ç±»å‹é€‰æ‹©ç›¸åº”çš„è§£ææ–¹æ³•
            if conference == 'ccs':
                papers = await self._parse_ccs_papers(self.conference_urls[conference], year)
            elif conference == 'sp':
                papers = await self._parse_sp_papers(year)
            elif conference == 'ndss':
                papers = await self._parse_ndss_papers(year)
            elif conference == 'usenix':
                papers = await self._parse_usenix_papers(year)

            if papers:
                print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                print(f"ğŸ“‹ æ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨:")
                for i, paper in enumerate(papers[:10]):
                    title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]
                    print(f"  {i+1:2d}. {title}{'...' if len(paper.get('title', '')) > 60 else ''}")
                
                if len(papers) > 10:
                    print(f"  ... å’Œå…¶ä»– {len(papers) - 10} ç¯‡è®ºæ–‡")
                
                # å¼€å§‹PDFé“¾æ¥éªŒè¯ä¸è¿›åº¦æ˜¾ç¤º
                print(f"\nğŸ”— æ­£åœ¨éªŒè¯PDFé“¾æ¥æœ‰æ•ˆæ€§...")
                valid_count = 0
                
                for i, paper in enumerate(papers):
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = (i + 1) / len(papers) * 100
                    progress_bar = 'â–ˆ' * int(progress // 5) + 'â–‘' * (20 - int(progress // 5))
                    print(f"\rğŸ“‹ [è¿›åº¦: {progress_bar}] {progress:.1f}% ({i+1}/{len(papers)}) éªŒè¯: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:30]}...", end='', flush=True)
                    
                    # æ£€æŸ¥URLæœ‰æ•ˆæ€§
                    if isinstance(paper.get('url'), str) and paper['url'].strip():
                        valid_count += 1
                
                print(f"\nâœ… PDFé“¾æ¥éªŒè¯å®Œæˆ: {valid_count}/{len(papers)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
            
            return papers
            
        except Exception as e:
            self.logger.error(f"è·å–è®ºæ–‡åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise
        """è·å–ä¼šè®®è®ºæ–‡åˆ—è¡¨ - å¸¦è¿›åº¦æ˜¾ç¤º"""
        try:
            if conference not in self.conference_urls:
                raise ValueError(f"ä¸æ”¯æŒçš„ä¼šè®®: {conference}")

            base_url = self.conference_urls[conference]
            papers = []
            
            print(f"ğŸ” æ­£åœ¨è·å– {conference.upper()} {year} è®ºæ–‡åˆ—è¡¨...")

            # è§„èŒƒåŒ–å¹´ä»½æ ¼å¼
            year = self.year_formats[conference](year)

            # æ ¹æ®ä¼šè®®ç±»å‹é€‰æ‹©ç›¸åº”çš„è§£ææ–¹æ³•
            papers = await self._get_papers_by_conference(conference, base_url, year)
            if papers:
                print(f"âœ¨ æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
            return papers

        except Exception as e:
            self.logger.error(f"è·å–è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            raise

    async def _get_papers_by_conference(self, conference: str, base_url: str, year: str) -> List[Dict[str, Any]]:
        """æ ¹æ®ä¼šè®®ç±»å‹è·å–è®ºæ–‡åˆ—è¡¨"""
        try:
            if conference == 'sp':
                # IEEE S&P
                full_url = f"{base_url}{year}"
                return await self._get_sp_papers(full_url, year)
            elif conference == 'ndss':
                # NDSS
                return await self._get_ndss_papers(base_url, year)
            elif conference == 'usenix':
                # USENIX Security
                return await self._get_usenix_papers(base_url, year)
            elif conference == 'ccs':
                # ACM CCS
                return await self._get_ccs_papers(base_url, year)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ä¼šè®®: {conference}")
        except Exception as e:
            self.logger.error(f"è·å–{conference.upper()} {year}è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            raise

    async def _get_sp_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è·å– IEEE S&P è®ºæ–‡åˆ—è¡¨"""
        papers = []
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/94.0.4606.81'
            }
            async with self.session.get(base_url, headers=headers) as response:
                if response.status_code != 200:
                    raise Exception(f"Failed to fetch SP {year} papers list")

                soup = BeautifulSoup(response.text, 'lxml')
                paper_items = soup.find_all('div', class_='article-list__item')

                for item in paper_items:
                    title_elem = item.find('h3', class_='article-list__title')
                    if not title_elem:
                        continue

                    title = title_elem.text.strip()
                    pdf_link = item.find('a', class_='pdf-link')
                    
                    if pdf_link and 'href' in pdf_link.attrs:
                        url = pdf_link['href']
                        if not url.startswith('http'):
                            url = f"https://www.computer.org{url}"
                        papers.append({
                            'title': title,
                            'url': url
                        })

        except Exception as e:
            self.logger.error(f"è§£æ SP {year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            raise

        return papers

    async def _get_ndss_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è·å– NDSS è®ºæ–‡åˆ—è¡¨"""
        papers = []
        try:
            url = f"{base_url}ndss{year}/accepted-papers"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/94.0.4606.81'
            }
            async with self.session.get(url, headers=headers) as response:
                if response.status_code != 200:
                    raise Exception(f"Failed to fetch NDSS {year} papers list")

                soup = BeautifulSoup(response.text, 'lxml')
                paper_items = soup.find_all('div', class_='paper-item')

                for item in paper_items:
                    title_elem = item.find('h2', class_='title')
                    if not title_elem:
                        continue

                    title = title_elem.text.strip()
                    pdf_link = item.find('a', href=lambda x: x and x.endswith('.pdf'))
                    
                    if pdf_link and 'href' in pdf_link.attrs:
                        url = pdf_link['href']
                        if not url.startswith('http'):
                            url = f"https://www.ndss-symposium.org{url}"
                        papers.append({
                            'title': title,
                            'url': url
                        })

        except Exception as e:
            self.logger.error(f"è§£æ NDSS {year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            raise

        return papers

    async def _get_usenix_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """è·å– USENIX Security è®ºæ–‡åˆ—è¡¨"""
        papers = []
        try:
            url = f"{base_url}{year}/technical-sessions"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/94.0.4606.81'
            }
            async with self.session.get(url, headers=headers) as response:
                if response.status_code != 200:
                    raise Exception(f"Failed to fetch USENIX {year} papers list")

                soup = BeautifulSoup(response.text, 'lxml')
                paper_items = soup.find_all('div', class_='node-paper')

                for item in paper_items:
                    title_elem = item.find('h2', class_='node-title')
                    if not title_elem:
                        continue

                    title = title_elem.text.strip()
                    pdf_link = item.find('a', href=lambda x: x and x.endswith('.pdf'))
                    
                    if pdf_link and 'href' in pdf_link.attrs:
                        url = pdf_link['href']
                        if not url.startswith('http'):
                            url = f"https://www.usenix.org{url}"
                        papers.append({
                            'title': title,
                            'url': url
                        })

        except Exception as e:
            self.logger.error(f"è§£æ USENIX {year} è®ºæ–‡åˆ—è¡¨å¤±è´¥: {e}")
            raise

        return papers

    async def get_papers(self, conference: str, year: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šä¼šè®®å’Œå¹´ä»½çš„è®ºæ–‡åˆ—è¡¨
        """
        try:
            base_url = self.conference_urls.get(conference, {}).get(year)
            if not base_url:
                self.logger.error(f"æœªæ‰¾åˆ° {conference} {year} çš„URLé…ç½®")
                return []

            if conference == 'ccs':
                papers = await self._parse_ccs_papers(base_url, year)
            elif conference == 'sp':
                papers = await self._parse_sp_papers(base_url, year)
            elif conference == 'ndss':
                papers = await self._parse_ndss_papers(base_url, year)
            elif conference == 'usenix':
                papers = await self._parse_usenix_papers(base_url, year)
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„ä¼šè®®ç±»å‹: {conference}")
                return []
            
            if papers:
                print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                print(f"ğŸ“‹ æ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨:")
                for i, paper in enumerate(papers[:10]):
                    title = paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]
                    print(f"  {i+1:2d}. {title}{'...' if len(paper.get('title', '')) > 60 else ''}")
                
                if len(papers) > 10:
                    print(f"  ... å’Œå…¶ä»– {len(papers) - 10} ç¯‡è®ºæ–‡")
                
                # å¼€å§‹PDFé“¾æ¥éªŒè¯ä¸è¿›åº¦æ˜¾ç¤º
                print(f"\nğŸ”— æ­£åœ¨éªŒè¯PDFé“¾æ¥æœ‰æ•ˆæ€§...")
                valid_count = 0
                
                for i, paper in enumerate(papers):
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = (i + 1) / len(papers) * 100
                    progress_bar = 'â–ˆ' * int(progress // 5) + 'â–‘' * (20 - int(progress // 5))
                    print(f"\rğŸ“‹ [è¿›åº¦: {progress_bar}] {progress:.1f}% ({i+1}/{len(papers)}) éªŒè¯: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')[:30]}...", end='', flush=True)
                    
                    # æ£€æŸ¥URLæœ‰æ•ˆæ€§
                    if isinstance(paper.get('url'), str) and paper['url'].strip():
                        valid_count += 1
                
                print(f"\nâœ… PDFé“¾æ¥éªŒè¯å®Œæˆ: {valid_count}/{len(papers)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            else:
                print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")

            return papers

        except Exception as e:
            self.logger.error(f"Error getting papers for {conference} {year}: {str(e)}")
            raise



    async def _parse_ccs_papers(self, base_url: str, year: str) -> List[Dict[str, Any]]:
        """
        è§£æCCSè®ºæ–‡åˆ—è¡¨çš„ä¸»å…¥å£å‡½æ•°ã€‚
        ä½¿ç”¨æŒä¹…åŒ–ä¼šè¯æ¥æ‰§è¡Œæ‰€æœ‰ç›¸å…³è¯·æ±‚ã€‚
        
        Args:
            base_url: è®ºæ–‡åˆ—è¡¨çš„åŸºç¡€URL
            year: ä¼šè®®å¹´ä»½
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []
        try:
            if not self.session:
                raise RuntimeError("æŒä¹…åŒ–ä¼šè¯æœªåˆå§‹åŒ–ã€‚è¯·åœ¨ 'async with' å—ä¸­ä½¿ç”¨ PaperDownloaderã€‚")
                
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            paper_dois = await self._get_all_ccs_dois_from_proceedings_page(self.session, year)
            if not paper_dois:
                self.logger.error(f"âŒ æœªèƒ½ä¸ºCCS {year} è·å–ä»»ä½•è®ºæ–‡çš„DOIã€‚")
                return []
            
            self.logger.info(f"ğŸ“š å¼€å§‹é€šè¿‡APIæ‰¹é‡è§£æ {len(paper_dois)} ç¯‡CCSè®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")
            
            papers = await self._fetch_all_ccs_paper_details_via_api(self.session, paper_dois, year)
            return papers
            
        except Exception as e:
            self.logger.error(f"âŒ CCSè®ºæ–‡è§£æä¸»æµç¨‹é”™è¯¯: {str(e)}")
            raise

    async def _fetch_all_ccs_paper_details_via_api(self, session: AsyncSession, dois: List[str], year: str) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨POSTè¯·æ±‚æ‰¹é‡è·å–æ‰€æœ‰CCSè®ºæ–‡çš„JSONæ•°æ®å¹¶è§£æã€‚
        """
        api_url = "https://dl.acm.org/action/exportCiteProcCitation"
        headers = {
            'Host': 'dl.acm.org',
            'Cookie': '_cf_bm=12; _cfuvid=eKvDTOvVWyHDD5bNf_GLEG_fzdrvwq1g_7YIL.aZOJU-1756624678973-0.0.1.1-604800000',
            'Pragma': 'no-cache',
            'Accept': '*/*',
            'Dnt': '1',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
            'Sec-Ch-Ua-Platform-Version': '"19.0.0"',
            'Origin': 'https://dl.acm.org',
            'Referer': 'https://dl.acm.org/doi/proceedings/10.1145/3658644',
            'Accept-Encoding': 'gzip, deflate, br',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Priority': 'u=1, i',
        }
        # åªä¿ç•™æœ€åä¸€æ®µæœ‰ç‚¹çš„DOIï¼ˆå³çœŸæ­£è®ºæ–‡ï¼‰
        filtered_dois = []
        for doi in dois:
            last = doi.split('/')[-1]
            if '.' in last:
                filtered_dois.append(doi)
        if not filtered_dois:
            self.logger.error("âŒ æ²¡æœ‰åˆæ ¼çš„DOIå¯ç”¨äºAPIè¯·æ±‚ï¼Œå…¨éƒ¨è¢«è¿‡æ»¤ã€‚")
            return []
        dois_payload = ",".join(filtered_dois)
        data_string = f"dois={dois_payload}&targetFile=custom-bibtex&format=json"
        content_length = str(len(data_string.encode('utf-8')))
        headers['Content-Length'] = content_length
        try:
            self.logger.info(f"ğŸš€ æ­£åœ¨å‘ {api_url} å‘é€å•æ¬¡POSTè¯·æ±‚ä»¥è·å– {len(dois)} ç¯‡è®ºæ–‡çš„JSONæ•°æ®...")
            response = await session.post(
                api_url,
                data=data_string,
                headers=headers,
                impersonate="chrome110",
                timeout=180
            )
            if response.status_code != 200:
                self.logger.error(f"âŒ æ‰¹é‡è·å–JSONå¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")
                self.logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                debug_file = Path(f"debug_ccs_api_error_{response.status_code}.html")
                debug_file.write_text(response.text, encoding='utf-8')
                self.logger.info(f"ğŸ› å·²å°†é”™è¯¯å“åº”ä¿å­˜åˆ° {debug_file.absolute()} ä»¥ä¾›è°ƒè¯•ã€‚")
                return []
            json_data_str = response.text
            self.logger.info(f"âœ… æˆåŠŸè·å–JSONæ•°æ®ï¼Œå¤§å°: {len(json_data_str)}å­—èŠ‚ã€‚å¼€å§‹è§£æ...")
            debug_json_file = Path("debug_ccs_json_response.json")
            debug_json_file.write_text(json_data_str, encoding='utf-8')
            self.logger.info(f"ğŸ› å·²å°†åŸå§‹JSONå“åº”ä¿å­˜åˆ° {debug_json_file.absolute()} ä»¥ä¾›åˆ†æã€‚")
            papers = self._parse_json_data(json_data_str, year)
            self.logger.info(f"âœ… æˆåŠŸè§£æ {len(papers)}/{len(dois)} ç¯‡è®ºæ–‡çš„å…ƒæ•°æ®ã€‚")
            return papers
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹é‡è·å–å’Œè§£æJSONæ•°æ®æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    def _parse_json_data(self, json_data_str: str, year: str) -> List[Dict[str, Any]]:
        """
        è§£æä»ACM APIè¿”å›çš„JSONæ•°æ®ã€‚
        """
        papers = []
        try:
            data = json.loads(json_data_str)
            for item_dict in data.get('items', []):
                for doi, details in item_dict.items():
                    try:
                        title = details.get('title', f"æœªçŸ¥æ ‡é¢˜ (DOI: {doi})")
                        authors_list = []
                        for author_info in details.get('author', []):
                            given_name = author_info.get('given', '')
                            family_name = author_info.get('family', '')
                            authors_list.append(f"{given_name} {family_name}".strip())
                        abstract = details.get('abstract', 'æ‘˜è¦ä¸å¯ç”¨')
                        pdf_url = f"https://dl.acm.org/doi/pdf/{doi}"
                        papers.append({
                            'title': title,
                            'authors': authors_list,
                            'abstract': abstract,
                            'url': pdf_url,
                            'conference': "CCS",
                            'year': year
                        })
                    except Exception as e:
                        self.logger.warning(f"è§£æå•ä¸ªJSONæ¡ç›®æ—¶å‡ºé”™ (DOI: {doi}): {e}")
                        continue
            return papers
        except json.JSONDecodeError as e:
            self.logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†JSONæ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []
  

    def _parse_bibtex_data(self, bibtex_data: str, year: str) -> List[Dict[str, Any]]:
        """
        è§£æBibTeXæ•°æ®å­—ç¬¦ä¸²å¹¶è¿”å›è®ºæ–‡åˆ—è¡¨ã€‚
        ä½¿ç”¨æ›´å¥å£®çš„æ­£åˆ™è¡¨è¾¾å¼æ¥å¤„ç†å¤æ‚çš„BibTeXæ ¼å¼ã€‚
        """
        papers = []
        # ä½¿ç”¨æ›´å¯é çš„æ–¹å¼åˆ†å‰²æ¡ç›®ï¼šæŒ‰æ¢è¡Œç¬¦åçš„'@'åˆ†å‰²
        entries = re.split(r'\n@', bibtex_data)
        
        for entry in entries:
            if not entry.strip() or not entry.startswith('inproceedings'):
                continue

            try:
                # å¥å£®çš„DOIæå–
                doi_match = re.search(r'doi\s*=\s*\{([^}]+)\}', entry, re.IGNORECASE)
                doi = doi_match.group(1).strip() if doi_match else "æœªçŸ¥DOI"

                # å¥å£®çš„æ ‡é¢˜æå–ï¼Œèƒ½å¤„ç†åµŒå¥—èŠ±æ‹¬å·
                title_match = re.search(r'title\s*=\s*\{((?:[^{}]|\{[^{}]*\})+)\}', entry, re.IGNORECASE)
                title = title_match.group(1).strip().replace("{", "").replace("}", "") if title_match else f"æœªçŸ¥æ ‡é¢˜ (DOI: {doi})"

                # å¥å£®çš„ä½œè€…æå–
                author_match = re.search(r'author\s*=\s*\{([^}]+)\}', entry, re.IGNORECASE)
                authors_str = author_match.group(1) if author_match else ""
                authors_list = [name.strip().replace("{", "").replace("}", "") for name in authors_str.split(' and ')]

                abstract = "æ‘˜è¦éœ€è®¿é—®è®ºæ–‡é¡µé¢æŸ¥çœ‹"
                pdf_url = f"https://dl.acm.org/doi/pdf/{doi}"

                papers.append({
                    'title': title,
                    'authors': authors_list,
                    'abstract': abstract,
                    'url': pdf_url,
                    'conference': "CCS",
                    'year': year
                })
            except Exception as e:
                self.logger.warning(f"è§£æå•ä¸ªBibTeXæ¡ç›®æ—¶å‡ºé”™: {e}\næ¡ç›®å†…å®¹: {entry[:300]}...")
                continue
        return papers

    async def _get_all_ccs_dois_from_proceedings_page(self, session: AsyncSession, year: str) -> Optional[List[str]]:
        """
        è·å–CCSä¼šè®®æŒ‡å®šå¹´ä»½æ‰€æœ‰è®ºæ–‡çš„DOIåˆ—è¡¨ã€‚
        ä½¿ç”¨ä¼ å…¥çš„æŒä¹…åŒ–ä¼šè¯ã€‚
        """
        if not CURL_CFFI_AVAILABLE:
            self.logger.error("âŒ curl_cffi æœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡ŒCCSè®ºæ–‡æŠ“å–ã€‚")
            return None

        try:
            short_year_str = f"'{year}"
            full_year_str = f"20{year}" if len(year) == 2 else year
            
            proceedings_list_url = 'https://dl.acm.org/conference/ccs/proceedings'
            self.logger.info(f"ğŸŒ æ­£åœ¨é€šè¿‡æŒä¹…åŒ–ä¼šè¯è®¿é—®CCSä¼šè®®åˆ—è¡¨é¡µé¢: {proceedings_list_url}")

            # ä½¿ç”¨æ›´å®Œæ•´çš„è¯·æ±‚å¤´æ¥æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
            headers = {
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Cache-Control': 'max-age=0',
                'Connection': 'keep-alive',
                'DNT': '1',
                'Host': 'dl.acm.org',
                'Sec-Ch-Ua': '"Chromium";v="116", "Not)A;Brand";v="24", "Microsoft Edge";v="116"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.0.0'
            }

            # å°è¯•å¤šæ¬¡ï¼Œä½¿ç”¨ä¸åŒçš„æµè§ˆå™¨æ¨¡æ‹Ÿé…ç½®
            for browser in ["chrome110", "chrome99", "chrome100", "safari15_3"]:
                try:
                    response = await session.get(
                        proceedings_list_url,
                        impersonate=browser,
                        headers=headers,
                        timeout=45
                    )
                    
                    if response.status_code == 200:
                        self.logger.info(f"âœ… ä½¿ç”¨ {browser} æˆåŠŸè®¿é—®")
                        break
                    else:
                        self.logger.warning(f"ä½¿ç”¨ {browser} å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    self.logger.warning(f"ä½¿ç”¨ {browser} æ—¶å‡ºé”™: {str(e)}")
                    await asyncio.sleep(2)  # å¤±è´¥åç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    continue
            
            if response.status_code != 200:
                self.logger.error(f"âŒ è®¿é—®CCSä¼šè®®åˆ—è¡¨é¡µé¢å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")
                return None
            
            self.logger.info("âœ… æˆåŠŸè·å–ä¼šè®®åˆ—è¡¨é¡µé¢ã€‚")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            target_proc_url = None
            proc_items = soup.select('li.conference__proceedings div.conference__title a')
            for item in proc_items:
                link_text = item.get_text(strip=True)
                if short_year_str in link_text or full_year_str in link_text:
                    target_proc_url = urllib.parse.urljoin(proceedings_list_url, item['href'])
                    self.logger.info(f"âœ… æ‰¾åˆ° CCS {year} ä¼šè®®å½•é“¾æ¥: {target_proc_url}")
                    break
            
            if not target_proc_url:
                self.logger.error(f"âŒ æœªèƒ½åœ¨é¡µé¢ä¸Šæ‰¾åˆ° CCS {year} çš„ä¼šè®®å½•é“¾æ¥ã€‚")
                debug_file = Path("debug_acm_proceedings_list.html")
                debug_file.write_text(response.text, encoding='utf-8')
                self.logger.info(f"ğŸ› å·²å°†ä¼šè®®åˆ—è¡¨é¡µé¢å†…å®¹ä¿å­˜åˆ° {debug_file.absolute()} ä»¥ä¾›è°ƒè¯•ã€‚")
                return None

            self.logger.info(f"ğŸŒ æ­£åœ¨è®¿é—® CCS {year} è®ºæ–‡åˆ—è¡¨é¡µé¢...")
            response = await session.get(target_proc_url, impersonate="chrome110", timeout=45)

            if response.status_code != 200:
                self.logger.error(f"âŒ è®¿é—® CCS {year} è®ºæ–‡åˆ—è¡¨é¡µé¢å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : {response.status_code}")
                return None
            
            self.logger.info(f"âœ… æˆåŠŸè·å– CCS {year} è®ºæ–‡åˆ—è¡¨é¡µé¢ã€‚")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            all_dois = []
            # ä»éšè—çš„inputä¸­æå–æ‰€æœ‰DOI
            doi_inputs = soup.select('input.section--dois')
            for doi_input in doi_inputs:
                dois_str = doi_input.get('value', '')
                if dois_str:
                    all_dois.extend(dois_str.split(','))

            if not all_dois:
                self.logger.warning(f"æœªèƒ½åœ¨ CCS {year} é¡µé¢æå–åˆ°ä»»ä½•è®ºæ–‡DOIã€‚è¯·æ£€æŸ¥é¡µé¢ç»“æ„æ˜¯å¦å·²æ›´æ”¹ã€‚")
                debug_file = Path(f"debug_ccs_{year}_papers.html")
                debug_file.write_text(response.text, encoding='utf-8')
                self.logger.info(f"ğŸ› å·²å°†è®ºæ–‡åˆ—è¡¨é¡µé¢å†…å®¹ä¿å­˜åˆ° {debug_file.absolute()} ä»¥ä¾›è°ƒè¯•ã€‚")
                return None
            
            # å»é‡å¹¶æ¸…æ´—
            unique_dois = sorted(list(set(doi.strip() for doi in all_dois if doi.strip())))
            self.logger.info(f"âœ… æˆåŠŸæå– {len(unique_dois)} ä¸ªå”¯ä¸€çš„ CCS {year} è®ºæ–‡DOIã€‚")
            return unique_dois

        except Exception as e:
            self.logger.error(f"âŒ è·å–CCSè®ºæ–‡DOIæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return None


