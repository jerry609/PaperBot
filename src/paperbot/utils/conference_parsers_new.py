import asyncio
from curl_cffi.requests import AsyncSession
from bs4 import BeautifulSoup
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging

def setup_logger(name):
    logger = logging.getLogger(name)
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger

class ConferenceParsers:
    def __init__(self):
        self.logger = setup_logger(__name__)

    async def parse_ndss_papers(self, base_url: str, year: str, session: AsyncSession) -> List[Dict[str, Any]]:
        """è§£æNDSSè®ºæ–‡åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬å¸¦è¿›åº¦æ˜¾ç¤º"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        url = f"{base_url}ndss{full_year}/accepted-papers/"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"ğŸŒ è®¿é—® NDSS {year} ä¼šè®®é¡µé¢...")
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with session.get(url, headers=headers) as response:
                    print(f"âš¡ å°è¯• {attempt + 1}/{max_retries}: HTTP {response.status}")
                    
                    if response.status == 200:
                        print(f"ğŸ“ æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # æŸ¥æ‰¾NDSSè®ºæ–‡å®¹å™¨
                        paper_containers = soup.find_all('div', class_='tag-box rel-paper')
                        print(f"ğŸ“š æ‰¾åˆ° {len(paper_containers)} ä¸ªè®ºæ–‡å®¹å™¨")
                        
                        if not paper_containers:
                            print(f"âš ï¸  æœªæ‰¾åˆ°è®ºæ–‡å®¹å™¨ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨...")
                            # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
                            paper_containers = soup.find_all('div', class_='paper') or soup.find_all('article')
                            print(f"ğŸ”„ å¤‡ç”¨é€‰æ‹©å™¨æ‰¾åˆ° {len(paper_containers)} ä¸ªå®¹å™¨")
                        
                        # å¤„ç†è®ºæ–‡å®¹å™¨å¹¶æ˜¾ç¤ºç®€å•è¿›åº¦
                        for idx, container in enumerate(paper_containers):
                            if idx % 5 == 0 or idx == len(paper_containers) - 1:  # æ¯5ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                progress = (idx + 1) / len(paper_containers) * 100
                                print(f"ï¿½ è§£æè¿›åº¦: {progress:.1f}% ({idx+1}/{len(paper_containers)})")
                            
                            try:
                                # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
                                title_elem = (container.find('h3', class_='blog-post-title') or 
                                            container.find('h3') or 
                                            container.find('h2') or 
                                            container.find('h1'))
                                
                                if not title_elem:
                                    continue
                                
                                title = title_elem.get_text().strip()
                                
                                # æ˜¾ç¤ºæ‰¾åˆ°çš„è®ºæ–‡æ ‡é¢˜
                                print(f"ğŸ“„ [{idx+1}/{len(paper_containers)}] æ‰¾åˆ°è®ºæ–‡: {title[:70]}{'...' if len(title) > 70 else ''}")
                                
                                # æå–ä½œè€…ä¿¡æ¯
                                author_elem = container.find('p')
                                authors_text = author_elem.get_text().strip() if author_elem else ''
                                authors = [author.strip() for author in authors_text.split(',')] if authors_text else []
                                
                                # æå–è¯¦æƒ…é¡µé“¾æ¥
                                detail_link = (container.find('a', class_='paper-link-abs') or 
                                            container.find('a', href=True))
                                detail_url = detail_link.get('href') if detail_link else ''
                                
                                # æå–PDFé“¾æ¥
                                pdf_url = ''
                                if detail_url:
                                    # å°†è¯¦æƒ…é¡µURLè½¬æ¢ä¸ºç»å¯¹URL
                                    if not detail_url.startswith('http'):
                                        if detail_url.startswith('//'):
                                            detail_url = f'https:{detail_url}'
                                        elif detail_url.startswith('/'):
                                            detail_url = f'https://www.ndss-symposium.org{detail_url}'
                                        else:
                                            detail_url = f'https://www.ndss-symposium.org/{detail_url}'
                                    pdf_url = await self._get_ndss_pdf_from_detail_page(session, detail_url)
                                
                                paper_info = {
                                    'title': title,
                                    'authors': authors,
                                    'abstract': '',
                                    'url': pdf_url,
                                    'detail_url': detail_url,
                                    'doi': ''
                                }
                                
                                if title and len(title) > 10:
                                    papers.append(paper_info)
                            except Exception as e:
                                self.logger.warning(f"Error parsing paper container {idx}: {str(e)}")
                                continue
                                
                        print(f"\nâœ… æˆåŠŸè§£æ {len(papers)} ç¯‡è®ºæ–‡")
                        return papers

            except Exception as e:
                print(f"âŒ å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    delay = 3 * (attempt + 1)
                    print(f"â³ ç­‰å¾… {delay} ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
                else:
                    return []
        
        print("âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return []

    async def _get_ndss_pdf_from_detail_page(self, session: AsyncSession, detail_url: str) -> str:
        """ä»NDSSè®ºæ–‡è¯¦æƒ…é¡µæå–PDFé“¾æ¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with session.get(detail_url, headers=headers) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # å°è¯•å¤šç§PDFé€‰æ‹©å™¨
                    pdf_selectors = [
                        ('a', {'href': lambda x: x and x.endswith('.pdf')}),
                        ('a', {'href': lambda x: x and 'paper' in x.lower() and '.pdf' in x.lower()}),
                        ('a', {'class': 'file-pdf'}),
                        ('a', {'class': 'download-pdf'}),
                        ('a', {'title': lambda x: x and 'pdf' in x.lower()})
                    ]
                    
                    for tag, attrs in pdf_selectors:
                        pdf_link = soup.find(tag, attrs)
                        if pdf_link and 'href' in pdf_link.attrs:
                            pdf_url = pdf_link['href']
                            # å¤„ç†ç›¸å¯¹URL
                            if not pdf_url.startswith('http'):
                                if pdf_url.startswith('//'):
                                    pdf_url = f'https:{pdf_url}'
                                elif pdf_url.startswith('/'):
                                    pdf_url = f'https://www.ndss-symposium.org{pdf_url}'
                                else:
                                    pdf_url = f'https://www.ndss-symposium.org/{pdf_url}'
                            return pdf_url
                            
            return ''
            
        except Exception as e:
            self.logger.warning(f"Error getting PDF from detail page: {str(e)}")
            return ''

    async def parse_usenix_papers(self, base_url: str, year: str, session: AsyncSession) -> List[Dict[str, Any]]:
        """è§£æUSENIX Securityè®ºæ–‡åˆ—è¡¨"""
        papers = []
        url = f"https://www.usenix.org/conference/usenixsecurity{year}/technical-sessions"
        
        print(f"ğŸŒ æ­£åœ¨è§£æ USENIX Security {year} è®ºæ–‡åˆ—è¡¨...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            response = await session.get(url, headers=headers)
            if response.status_code == 200:
                print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸï¼Œå¼€å§‹è§£æ...")
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                    
                # ä½¿ç”¨å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾è®ºæ–‡èŠ‚ç‚¹
                paper_nodes = soup.find_all(['article', 'div'], class_=['node-paper', 'paper-item'])
                
                print(f"ğŸ“š æ‰¾åˆ° {len(paper_nodes)} ä¸ªè®ºæ–‡èŠ‚ç‚¹")
                
                if not paper_nodes:
                    print("âš ï¸ æœªæ‰¾åˆ°è®ºæ–‡èŠ‚ç‚¹ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨...")
                    paper_nodes = soup.find_all(['div', 'article'], class_=['paper', 'technical-paper'])
                
                for idx, node in enumerate(paper_nodes, 1):
                    try:
                        # æŸ¥æ‰¾æ ‡é¢˜
                        title_elem = node.find(['h2', 'h3'], class_=['node-title', 'paper-title']) or \
                                   node.find('div', class_='field-title')
                        if not title_elem:
                            continue
                        
                        title = title_elem.text.strip()
                        
                        # æŸ¥æ‰¾PDFé“¾æ¥
                        pdf_url = await self._get_usenix_pdf_url(node)
                        if pdf_url:
                            papers.append({
                                'title': title,
                                'url': pdf_url,
                                'conference': 'USENIX',
                                'year': year
                            })
                            
                            print(f"\rğŸ“„ å¤„ç†è®ºæ–‡ {idx}/{len(paper_nodes)}: {title[:50]}...", end='', flush=True)
                
                print(f"\nâœ… USENIXè§£æå®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
                return papers
                
            elif response.status_code == 404:
                print(f"âŒ USENIX {year} é¡µé¢ä¸å­˜åœ¨")
                return []
            else:
                raise Exception(f"HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"âŒ USENIXè§£æé”™è¯¯: {str(e)}")
            return []

    async def _get_usenix_pdf_url(self, node) -> Optional[str]:
        """ä»USENIXè®ºæ–‡èŠ‚ç‚¹è·å–PDFé“¾æ¥"""
        try:
            # ç›´æ¥æŸ¥æ‰¾PDFé“¾æ¥
            pdf_link = node.find('a', href=re.compile(r'\.pdf$', re.I))
            if pdf_link and pdf_link.get('href'):
                pdf_url = pdf_link['href']
                return self._complete_usenix_url(pdf_url)
            
            # æŸ¥æ‰¾presentationé“¾æ¥
            pres_link = node.find('a', href=re.compile(r'/presentation/', re.I))
            if pres_link and pres_link.get('href'):
                pres_url = pres_link['href']
                return self._complete_usenix_url(pres_url)
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ PDFé“¾æ¥æå–å¤±è´¥: {str(e)}")
            return None

    def _complete_usenix_url(self, url: str) -> str:
        """è¡¥å…¨USENIX URL"""
        if not url:
            return ''
        
        if url.startswith('http'):
            return url
        elif url.startswith('//'):
            return f"https:{url}"
        elif url.startswith('/'):
            return f"https://www.usenix.org{url}"
        else:
            return f"https://www.usenix.org/{url}"

    async def parse_sp_papers(self, base_url: str, year: str, session: AsyncSession) -> List[Dict[str, Any]]:
        """è§£æIEEE S&Pè®ºæ–‡åˆ—è¡¨"""
        papers = []
        full_year = f"20{year}" if len(year) == 2 else year
        
        print(f"ğŸŒ æ­£åœ¨è·å– IEEE S&P {full_year} è®ºæ–‡åˆ—è¡¨...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1'
        }
        
        try:
            response = await session.get(base_url, headers=headers)
            if response.status_code == 200:
                print(f"ğŸ“ æ­£åœ¨è§£æé¡µé¢å†…å®¹...")
                html = response.text
                soup = BeautifulSoup(html, 'html.parser')
                    
                # æŸ¥æ‰¾è®ºæ–‡é¡¹
                paper_items = soup.find_all(['div', 'article'], class_=['paper-item', 'article-item'])
                print(f"ğŸ“š æ‰¾åˆ° {len(paper_items)} ä¸ªè®ºæ–‡é¡¹")
                
                if not paper_items:
                    paper_items = soup.find_all(['div', 'article'], class_=['paper', 'article'])
                
                for idx, item in enumerate(paper_items, 1):
                    try:
                        # æŸ¥æ‰¾æ ‡é¢˜
                        title_elem = item.find(['h3', 'h2'], class_=['paper-title', 'article-title'])
                        if not title_elem:
                            continue
                            
                        title = title_elem.text.strip()
                        
                        # æŸ¥æ‰¾PDFé“¾æ¥
                        pdf_url = await self._get_ieee_pdf_url(item)
                        if pdf_url:
                            papers.append({
                                'title': title,
                                'url': pdf_url,
                                'conference': 'SP',
                                'year': year
                            })
                            
                            print(f"\rğŸ“„ å¤„ç†è®ºæ–‡ {idx}/{len(paper_items)}: {title[:50]}...", end='', flush=True)
                    except Exception as e:
                        print(f"\nâš ï¸ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                        continue
                            
                    except Exception as e:
                        print(f"\nâš ï¸ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                print(f"\nâœ… SPè§£æå®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
                return papers
                    
            else:
                raise Exception(f"HTTP {response.status_code}")
                    
        except Exception as e:
            print(f"âŒ SPè§£æé”™è¯¯: {str(e)}")
            return []

    async def _get_ieee_pdf_url(self, paper_element) -> Optional[str]:
        """ä»IEEEè®ºæ–‡å…ƒç´ ä¸­æå–PDF URL"""
        try:
            # ç›´æ¥æŸ¥æ‰¾PDFé“¾æ¥
            pdf_link = paper_element.find('a', href=re.compile(r'\.pdf$', re.I))
            if pdf_link and pdf_link.get('href'):
                return pdf_link['href']
            
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥å¹¶æ„é€ PDF URL
            article_link = paper_element.find('a', href=re.compile(r'/document/'))
            if article_link and article_link.get('href'):
                doc_match = re.search(r'/document/(\d+)', article_link['href'])
                if doc_match:
                    doc_id = doc_match.group(1)
                    return f"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_id}"
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ PDFé“¾æ¥æå–å¤±è´¥: {str(e)}")
            return None
