"""
Deep Research å­¦è€…è¿½è¸ª Agent

å®ç° BettaFish InsightEngine çš„ Deep Research æ¨¡å¼
é€šè¿‡è¿­ä»£åæ€å¾ªç¯æ·±å…¥åˆ†æå­¦è€…ä¿¡æ¯

å‚è€ƒ: BettaFish/InsightEngine/insight_engine.py
"""

from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime
from loguru import logger

from agents.base_agent import BaseAgent
from core.llm_client import LLMClient
from core.state import TrackingState, ScholarState, PaperState, InfluenceState
from scholar_tracking.nodes import (
    ScholarFetchNode,
    PaperDetectionNode,
    InfluenceCalculationNode,
    ReportGenerationNode,
    ReflectionSearchNode,
    ReflectionSummaryNode,
)
from prompts.scholar_prompts import (
    SYSTEM_PROMPT_ANALYZE_PAPER,
    SYSTEM_PROMPT_ASSESS_INFLUENCE,
)
from utils.json_parser import parse_json


@dataclass
class DeepResearchConfig:
    """Deep Research é…ç½®"""
    max_reflections: int = 3  # æœ€å¤§åæ€æ¬¡æ•°
    min_completeness_score: float = 0.8  # æœ€å°ä¿¡æ¯å®Œæ•´åº¦
    max_papers_per_search: int = 20  # æ¯æ¬¡æœç´¢æœ€å¤šè·å–è®ºæ–‡æ•°
    enable_code_analysis: bool = True  # æ˜¯å¦åˆ†æä»£ç ä»“åº“
    enable_citation_analysis: bool = True  # æ˜¯å¦åˆ†æå¼•ç”¨å…³ç³»


@dataclass
class ResearchState:
    """Deep Research çŠ¶æ€"""
    scholar_info: Dict[str, Any] = field(default_factory=dict)
    collected_papers: List[Dict[str, Any]] = field(default_factory=list)
    current_summary: str = ""
    key_findings: List[str] = field(default_factory=list)
    completeness_score: float = 0.0
    reflection_count: int = 0
    search_history: List[Dict[str, Any]] = field(default_factory=list)
    influence_result: Optional[Dict[str, Any]] = None


class DeepResearchAgent(BaseAgent):
    """
    Deep Research å­¦è€…è¿½è¸ª Agent
    
    å®ç°è¿­ä»£åæ€å¾ªç¯:
    1. åˆå§‹æœç´¢ - è·å–å­¦è€…åŸºæœ¬ä¿¡æ¯å’Œè®ºæ–‡
    2. æ€»ç»“ - æ•´ç†å·²æ”¶é›†ä¿¡æ¯
    3. åæ€ - è¯†åˆ«ä¿¡æ¯ç©ºç™½
    4. è¡¥å……æœç´¢ - å¡«è¡¥ç©ºç™½
    5. æ›´æ–°æ€»ç»“ - æ•´åˆæ–°ä¿¡æ¯
    6. é‡å¤ 3-5 ç›´åˆ°ä¿¡æ¯å……åˆ†æˆ–è¾¾åˆ°æœ€å¤§æ¬¡æ•°
    7. ç”ŸæˆæŠ¥å‘Š - è¾“å‡ºæœ€ç»ˆåˆ†æç»“æœ
    """
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        semantic_scholar_agent: Any = None,
        scholar_profile_agent: Any = None,
        influence_calculator: Any = None,
        config: Optional[DeepResearchConfig] = None,
    ):
        """
        åˆå§‹åŒ– Deep Research Agent
        
        Args:
            llm_client: LLM å®¢æˆ·ç«¯
            semantic_scholar_agent: Semantic Scholar API Agent
            scholar_profile_agent: å­¦è€…æ¡£æ¡ˆç®¡ç† Agent
            influence_calculator: å½±å“åŠ›è®¡ç®—å™¨
            config: ç ”ç©¶é…ç½®
        """
        super().__init__()
        
        self.llm_client = llm_client
        self.semantic_scholar_agent = semantic_scholar_agent
        self.scholar_profile_agent = scholar_profile_agent
        self.influence_calculator = influence_calculator
        self.config = config or DeepResearchConfig()
        
        # åˆå§‹åŒ–èŠ‚ç‚¹
        self._init_nodes()
    
    def _init_nodes(self):
        """åˆå§‹åŒ–å¤„ç†èŠ‚ç‚¹"""
        if self.llm_client:
            self.reflection_search_node = ReflectionSearchNode(
                llm_client=self.llm_client,
                max_reflections=self.config.max_reflections,
            )
            self.reflection_summary_node = ReflectionSummaryNode(
                llm_client=self.llm_client,
            )
        else:
            self.reflection_search_node = None
            self.reflection_summary_node = None
        
        if self.scholar_profile_agent:
            self.scholar_fetch_node = ScholarFetchNode(
                scholar_profile=self.scholar_profile_agent,
            )
        else:
            self.scholar_fetch_node = None
        
        if self.influence_calculator:
            self.influence_node = InfluenceCalculationNode(
                calculator=self.influence_calculator,
            )
        else:
            self.influence_node = None
    
    async def process(self, *args, **kwargs) -> Dict[str, Any]:
        """
        BaseAgent è¦æ±‚çš„å¤„ç†æ–¹æ³•
        """
        action = kwargs.get("action", "research")
        
        if action == "research":
            scholar_id = kwargs.get("scholar_id")
            analysis_goal = kwargs.get("goal", "å…¨é¢åˆ†æå­¦è€…çš„å­¦æœ¯å½±å“åŠ›")
            return await self.deep_research(scholar_id, analysis_goal)
        
        return {"error": "Unknown action"}
    
    async def deep_research(
        self,
        scholar_id: str,
        analysis_goal: str = "å…¨é¢åˆ†æå­¦è€…çš„å­¦æœ¯å½±å“åŠ›",
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦ç ”ç©¶
        
        Args:
            scholar_id: å­¦è€… ID (Semantic Scholar ID)
            analysis_goal: åˆ†æç›®æ ‡
            
        Returns:
            åŒ…å«å®Œæ•´åˆ†æç»“æœçš„å­—å…¸
        """
        logger.info(f"ğŸ”¬ å¼€å§‹æ·±åº¦ç ”ç©¶: {scholar_id}")
        logger.info(f"ğŸ“‹ åˆ†æç›®æ ‡: {analysis_goal}")
        
        # åˆå§‹åŒ–ç ”ç©¶çŠ¶æ€
        state = ResearchState()
        
        try:
            # Step 1: åˆå§‹æœç´¢
            logger.info("ğŸ“¡ Step 1: åˆå§‹æœç´¢ - è·å–å­¦è€…ä¿¡æ¯å’Œè®ºæ–‡")
            await self._initial_search(state, scholar_id)
            
            if not state.scholar_info:
                logger.error(f"âŒ æ— æ³•è·å–å­¦è€…ä¿¡æ¯: {scholar_id}")
                return {"error": f"æ— æ³•è·å–å­¦è€…ä¿¡æ¯: {scholar_id}"}
            
            # Step 2: åˆå§‹æ€»ç»“
            logger.info("ğŸ“ Step 2: åˆå§‹æ€»ç»“")
            await self._initial_summary(state)
            
            # Step 3-5: åæ€å¾ªç¯
            if self.reflection_search_node and self.reflection_summary_node:
                logger.info("ğŸ”„ Step 3-5: åæ€å¾ªç¯")
                await self._reflection_loop(state, analysis_goal)
            
            # Step 6: å½±å“åŠ›è¯„ä¼°
            logger.info("ğŸ“Š Step 6: å½±å“åŠ›è¯„ä¼°")
            await self._assess_influence(state)
            
            # Step 7: ç”ŸæˆæŠ¥å‘Š
            logger.info("ğŸ“‘ Step 7: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
            result = self._generate_final_result(state, analysis_goal)
            
            logger.success(f"âœ… æ·±åº¦ç ”ç©¶å®Œæˆ: {state.scholar_info.get('name', scholar_id)}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ·±åº¦ç ”ç©¶å¤±è´¥: {e}")
            return {
                "error": str(e),
                "partial_state": {
                    "scholar_info": state.scholar_info,
                    "papers_collected": len(state.collected_papers),
                    "completeness": state.completeness_score,
                },
            }
    
    async def _initial_search(self, state: ResearchState, scholar_id: str):
        """
        æ‰§è¡Œåˆå§‹æœç´¢
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
            scholar_id: å­¦è€… ID
        """
        # è·å–å­¦è€…ä¿¡æ¯
        if self.semantic_scholar_agent:
            try:
                scholar_data = await self._fetch_scholar_info(scholar_id)
                state.scholar_info = scholar_data if scholar_data else {}
            except Exception as e:
                logger.warning(f"è·å–å­¦è€…ä¿¡æ¯å¤±è´¥: {e}")
                state.scholar_info = {"authorId": scholar_id}
        
        # è·å–å­¦è€…è®ºæ–‡
        if self.semantic_scholar_agent and state.scholar_info:
            try:
                papers = await self._fetch_scholar_papers(scholar_id)
                state.collected_papers = papers[:self.config.max_papers_per_search]
                logger.info(f"  â†’ è·å–åˆ° {len(state.collected_papers)} ç¯‡è®ºæ–‡")
            except Exception as e:
                logger.warning(f"è·å–è®ºæ–‡å¤±è´¥: {e}")
        
        # è®°å½•æœç´¢å†å²
        state.search_history.append({
            "type": "initial",
            "query": scholar_id,
            "timestamp": datetime.now().isoformat(),
            "results_count": len(state.collected_papers),
        })
    
    async def _fetch_scholar_info(self, scholar_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å­¦è€…ä¿¡æ¯"""
        if hasattr(self.semantic_scholar_agent, 'get_author'):
            return await self.semantic_scholar_agent.get_author(scholar_id)
        elif hasattr(self.semantic_scholar_agent, 'fetch_author_info'):
            return self.semantic_scholar_agent.fetch_author_info(scholar_id)
        return None
    
    async def _fetch_scholar_papers(self, scholar_id: str) -> List[Dict[str, Any]]:
        """è·å–å­¦è€…è®ºæ–‡"""
        if hasattr(self.semantic_scholar_agent, 'get_author_papers'):
            return await self.semantic_scholar_agent.get_author_papers(scholar_id)
        elif hasattr(self.semantic_scholar_agent, 'fetch_recent_papers'):
            return self.semantic_scholar_agent.fetch_recent_papers(scholar_id)
        return []
    
    async def _initial_summary(self, state: ResearchState):
        """
        ç”Ÿæˆåˆå§‹æ€»ç»“
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
        """
        scholar = state.scholar_info
        papers = state.collected_papers
        
        # ç”ŸæˆåŸºæœ¬æ€»ç»“
        summary_parts = []
        
        if scholar.get('name'):
            summary_parts.append(f"å­¦è€…: {scholar['name']}")
        
        if scholar.get('affiliations'):
            affiliations = scholar['affiliations']
            if isinstance(affiliations, list):
                aff_str = ', '.join([a.get('name', str(a)) if isinstance(a, dict) else str(a) for a in affiliations[:3]])
            else:
                aff_str = str(affiliations)
            summary_parts.append(f"æœºæ„: {aff_str}")
        
        h_index = scholar.get('h_index') or scholar.get('hIndex', 0)
        citation_count = scholar.get('citation_count') or scholar.get('citationCount', 0)
        
        summary_parts.append(f"H-index: {h_index}, æ€»å¼•ç”¨: {citation_count}")
        summary_parts.append(f"å·²è·å– {len(papers)} ç¯‡è®ºæ–‡")
        
        if papers:
            recent_papers = sorted(papers, key=lambda p: p.get('year', 0), reverse=True)[:3]
            summary_parts.append("è¿‘æœŸè®ºæ–‡:")
            for p in recent_papers:
                summary_parts.append(f"  - {p.get('title', 'æœªçŸ¥æ ‡é¢˜')[:60]}... ({p.get('year', 'N/A')})")
        
        state.current_summary = "\n".join(summary_parts)
        state.completeness_score = 0.4  # åˆå§‹å®Œæ•´åº¦
        
        logger.info(f"  â†’ åˆå§‹å®Œæ•´åº¦: {state.completeness_score:.1%}")
    
    async def _reflection_loop(self, state: ResearchState, analysis_goal: str):
        """
        æ‰§è¡Œåæ€å¾ªç¯
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
            analysis_goal: åˆ†æç›®æ ‡
        """
        while state.reflection_count < self.config.max_reflections:
            logger.info(f"  ğŸ”„ åæ€ {state.reflection_count + 1}/{self.config.max_reflections}")
            
            # æ‰§è¡Œåæ€æœç´¢
            reflection_input = {
                "scholar_info": state.scholar_info,
                "collected_papers": state.collected_papers,
                "current_summary": state.current_summary,
                "reflection_count": state.reflection_count,
                "analysis_goal": analysis_goal,
            }
            
            reflection_result = self.reflection_search_node.run(reflection_input)
            
            if not reflection_result.get("should_continue"):
                logger.info(f"  â†’ åæ€å¾ªç¯ç»“æŸ: {reflection_result.get('reason', 'ä¿¡æ¯å·²å……åˆ†')}")
                break
            
            # æ‰§è¡Œè¡¥å……æœç´¢
            search_query = reflection_result.get("search_query", "")
            search_type = reflection_result.get("search_type", "papers")
            
            logger.info(f"  â†’ è¡¥å……æœç´¢ ({search_type}): {search_query[:50]}...")
            
            new_results = await self._supplementary_search(search_query, search_type, state)
            
            # æ›´æ–°æ€»ç»“
            if new_results:
                summary_input = {
                    "current_summary": state.current_summary,
                    "new_results": new_results,
                    "scholar_info": state.scholar_info,
                }
                
                summary_result = self.reflection_summary_node.run(summary_input)
                
                state.current_summary = summary_result.get("updated_summary", state.current_summary)
                state.key_findings.extend(summary_result.get("key_findings", []))
                state.completeness_score = summary_result.get("completeness_score", state.completeness_score)
                
                logger.info(f"  â†’ æ›´æ–°å®Œæ•´åº¦: {state.completeness_score:.1%}")
            
            # æ›´æ–°åæ€è®¡æ•°
            state.reflection_count = reflection_result.get("reflection_count", state.reflection_count + 1)
            
            # è®°å½•æœç´¢å†å²
            state.search_history.append({
                "type": search_type,
                "query": search_query,
                "timestamp": datetime.now().isoformat(),
                "results_count": len(new_results),
                "gaps": reflection_result.get("gaps_identified", []),
            })
            
            # æ£€æŸ¥å®Œæ•´åº¦
            if state.completeness_score >= self.config.min_completeness_score:
                logger.info(f"  â†’ è¾¾åˆ°ç›®æ ‡å®Œæ•´åº¦ ({self.config.min_completeness_score:.0%})ï¼Œåœæ­¢åæ€")
                break
    
    async def _supplementary_search(
        self,
        query: str,
        search_type: str,
        state: ResearchState,
    ) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œè¡¥å……æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            search_type: æœç´¢ç±»å‹
            state: ç ”ç©¶çŠ¶æ€
            
        Returns:
            æ–°æœç´¢ç»“æœ
        """
        results = []
        
        try:
            if search_type == "papers" and self.semantic_scholar_agent:
                # æœç´¢æ›´å¤šè®ºæ–‡
                if hasattr(self.semantic_scholar_agent, 'search_papers'):
                    new_papers = await self.semantic_scholar_agent.search_papers(query)
                    # å»é‡
                    existing_ids = {p.get('paperId', p.get('paper_id')) for p in state.collected_papers}
                    for paper in new_papers:
                        paper_id = paper.get('paperId', paper.get('paper_id'))
                        if paper_id and paper_id not in existing_ids:
                            state.collected_papers.append(paper)
                            results.append(paper)
                            existing_ids.add(paper_id)
            
            elif search_type == "code" and self.config.enable_code_analysis:
                # æœç´¢ä»£ç ä»“åº“ (æœªæ¥æ‰©å±•)
                logger.info("  â†’ ä»£ç æœç´¢åŠŸèƒ½å¾…å®ç°")
            
            elif search_type == "citations" and self.config.enable_citation_analysis:
                # æœç´¢å¼•ç”¨å…³ç³» (æœªæ¥æ‰©å±•)
                logger.info("  â†’ å¼•ç”¨åˆ†æåŠŸèƒ½å¾…å®ç°")
            
            elif search_type == "collaborators":
                # æœç´¢åˆä½œè€… (æœªæ¥æ‰©å±•)
                logger.info("  â†’ åˆä½œè€…åˆ†æåŠŸèƒ½å¾…å®ç°")
                
        except Exception as e:
            logger.warning(f"  âš  è¡¥å……æœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def _assess_influence(self, state: ResearchState):
        """
        è¯„ä¼°å½±å“åŠ›
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
        """
        if not self.influence_node:
            logger.warning("  âš  å½±å“åŠ›è®¡ç®—èŠ‚ç‚¹æœªé…ç½®")
            return
        
        try:
            influence_input = {
                "scholars": [state.scholar_info],
                "papers": state.collected_papers,
            }
            
            influence_result = self.influence_node.run(influence_input)
            results = influence_result.get("influence_results", [])
            
            if results:
                state.influence_result = results[0]
                logger.info(f"  â†’ å½±å“åŠ›è¯„åˆ†: {state.influence_result.get('total_score', 0):.2f}")
        
        except Exception as e:
            logger.warning(f"  âš  å½±å“åŠ›è¯„ä¼°å¤±è´¥: {e}")
    
    def _generate_final_result(
        self,
        state: ResearchState,
        analysis_goal: str,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœ€ç»ˆç»“æœ
        
        Args:
            state: ç ”ç©¶çŠ¶æ€
            analysis_goal: åˆ†æç›®æ ‡
            
        Returns:
            å®Œæ•´åˆ†æç»“æœ
        """
        scholar = state.scholar_info
        
        return {
            "success": True,
            "scholar": {
                "id": scholar.get('authorId', scholar.get('scholar_id')),
                "name": scholar.get('name', 'æœªçŸ¥'),
                "affiliations": scholar.get('affiliations', []),
                "h_index": scholar.get('hIndex', scholar.get('h_index', 0)),
                "citation_count": scholar.get('citationCount', scholar.get('citation_count', 0)),
            },
            "analysis": {
                "summary": state.current_summary,
                "key_findings": list(set(state.key_findings)),  # å»é‡
                "completeness_score": state.completeness_score,
            },
            "papers": {
                "total_collected": len(state.collected_papers),
                "sample": state.collected_papers[:5],  # è¿”å›å‰5ç¯‡ä½œä¸ºæ ·æœ¬
            },
            "influence": state.influence_result or {},
            "metadata": {
                "analysis_goal": analysis_goal,
                "reflection_count": state.reflection_count,
                "search_history": state.search_history,
                "timestamp": datetime.now().isoformat(),
            },
        }
    
    def research_sync(
        self,
        scholar_id: str,
        analysis_goal: str = "å…¨é¢åˆ†æå­¦è€…çš„å­¦æœ¯å½±å“åŠ›",
    ) -> Dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æ·±åº¦ç ”ç©¶ (ç”¨äºéå¼‚æ­¥ç¯å¢ƒ)
        
        Args:
            scholar_id: å­¦è€… ID
            analysis_goal: åˆ†æç›®æ ‡
            
        Returns:
            åˆ†æç»“æœ
        """
        import asyncio
        
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.deep_research(scholar_id, analysis_goal))


# ä¾¿æ·å‡½æ•°
def create_deep_research_agent(
    api_key: Optional[str] = None,
    model: str = "gpt-4o-mini",
    **kwargs,
) -> DeepResearchAgent:
    """
    åˆ›å»º Deep Research Agent çš„ä¾¿æ·å‡½æ•°
    
    Args:
        api_key: OpenAI API Key
        model: æ¨¡å‹åç§°
        **kwargs: å…¶ä»–é…ç½®å‚æ•°
        
    Returns:
        é…ç½®å¥½çš„ DeepResearchAgent å®ä¾‹
    """
    llm_client = None
    if api_key:
        from core.llm_client import LLMClient
        llm_client = LLMClient(api_key=api_key, model=model)
    
    config = DeepResearchConfig(
        max_reflections=kwargs.get('max_reflections', 3),
        min_completeness_score=kwargs.get('min_completeness_score', 0.8),
        max_papers_per_search=kwargs.get('max_papers_per_search', 20),
    )
    
    return DeepResearchAgent(
        llm_client=llm_client,
        config=config,
    )
