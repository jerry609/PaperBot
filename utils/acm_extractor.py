#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆACMè®ºæ–‡æå–å™¨
ä¸“ä¸ºdownloaderæ¨¡å—è®¾è®¡
"""

import cloudscraper
import time
import random
import urllib3
import json
import gzip
import io
import brotli
from bs4 import BeautifulSoup
import re

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class ACMPaperExtractor:
    """ç®€åŒ–ç‰ˆACMè®ºæ–‡æå–å™¨"""
    
    def __init__(self):
        self.base_url = "https://dl.acm.org"
        self.scraper = None
        self._init_scraper()
        
    def _init_scraper(self):
        """åˆå§‹åŒ–cloudscraper"""
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'mobile': False
            },
            delay=10
        )
        
        # è®¾ç½®åŸºç¡€headers
        self.scraper.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'sec-ch-ua': '"Microsoft Edge";v="139", "Chromium";v="139", "Not A(Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        })
    
    def get_homepage(self):
        """è®¿é—®ä¸»é¡µè·å–cookies"""
        print("ğŸ  è®¿é—®ACMä¸»é¡µè·å–cookies...")
        
        try:
            response = self.scraper.get(self.base_url, timeout=30)
            print(f"ä¸»é¡µçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                print("âœ… æˆåŠŸè·å–ä¸»é¡µcookies")
                return True
            else:
                print(f"âŒ è·å–ä¸»é¡µå¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ è®¿é—®ä¸»é¡µå¼‚å¸¸: {str(e)}")
            return False
    
    def get_proceedings_page(self, proceedings_doi):
        """è®¿é—®proceedingsé¡µé¢è·å–cookieså’Œå¿…è¦çš„ä¸Šä¸‹æ–‡"""
        print(f"ğŸ“‚ è®¿é—®CCS proceedingsé¡µé¢...")
        
        if not self.get_homepage():
            return None
        
        url = f"{self.base_url}/doi/proceedings/{proceedings_doi}"
        
        # æ·»åŠ å»¶è¿Ÿ
        time.sleep(random.uniform(2, 5))
        
        try:
            response = self.scraper.get(url, timeout=30)
            print(f"Proceedingsé¡µé¢çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                print("âœ… æˆåŠŸè®¿é—®proceedingsé¡µé¢")
                return response.text
            else:
                print(f"âŒ è®¿é—®proceedingsé¡µé¢å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è®¿é—®proceedingsé¡µé¢å¼‚å¸¸: {str(e)}")
            return None
    
    def extract_all_paper_dois(self, proceedings_content):
        """ä»proceedingsé¡µé¢å†…å®¹ä¸­æå–æ‰€æœ‰è®ºæ–‡DOI"""
        print("ğŸ” ä»proceedingsé¡µé¢æå–æ‰€æœ‰è®ºæ–‡DOI...")
        
        if not proceedings_content:
            print("âŒ proceedingsé¡µé¢å†…å®¹ä¸ºç©º")
            return []
        
        try:
            soup = BeautifulSoup(proceedings_content, 'html.parser')
            dois = []
            
            # æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡é“¾æ¥
            paper_links = soup.find_all('a', href=re.compile(r'/doi/10\.1145/'))
            
            for link in paper_links:
                href = link.get('href', '')
                # ä»hrefä¸­æå–DOI
                doi_match = re.search(r'/doi/([^/]+/[^/?]+)', href)
                if doi_match:
                    doi = doi_match.group(1)
                    # ç¡®ä¿DOIæ ¼å¼æ­£ç¡®ä¸”ä¸é‡å¤
                    if doi.startswith('10.1145/') and doi not in dois:
                        dois.append(doi)
            
            print(f"âœ… æˆåŠŸæå–åˆ° {len(dois)} ä¸ªè®ºæ–‡DOI")
            return dois
            
        except Exception as e:
            print(f"âŒ æå–è®ºæ–‡DOIæ—¶å‡ºé”™: {str(e)}")
            return []
    
    def export_citations(self, doi_list, proceedings_doi):
        """ä½¿ç”¨export citation APIå¯¼å‡ºå¼•ç”¨ä¿¡æ¯"""
        print("ğŸ“š ä½¿ç”¨export citation APIå¯¼å‡ºå¼•ç”¨ä¿¡æ¯...")
        
        # ç¡®ä¿å·²è®¿é—®ä¸»é¡µå’Œproceedingsé¡µé¢
        self.get_proceedings_page(proceedings_doi)
        
        # æ„å»ºAPI URL
        api_url = f"{self.base_url}/action/exportCiteProcCitation"
        
        # æ ¼å¼åŒ–DOIåˆ—è¡¨
        formatted_doi_list = []
        for doi in doi_list:
            if doi.startswith('10.1145/'):
                formatted_doi_list.append(doi)
            else:
                formatted_doi_list.append(f"10.1145/{doi}")
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            'dois': ','.join(formatted_doi_list),
            'targetFile': 'custom-bibtex',
            'format': 'bibTex'
        }
        
        # è®¾ç½®APIè¯·æ±‚headers
        api_headers = {
            'X-Requested-With': 'XMLHttpRequest',
            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            'Accept': '*/*',
            'Origin': self.base_url,
            'Referer': f"{self.base_url}/doi/proceedings/{proceedings_doi}",
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Dest': 'empty',
            'DNT': '1',
            'sec-ch-ua': '"Microsoft Edge";v="139", "Chromium";v="139", "Not A(Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        }
        
        # æ›´æ–°è¯·æ±‚å¤´
        self.scraper.headers.update(api_headers)
        
        # æ·»åŠ å»¶è¿Ÿ
        time.sleep(random.uniform(2, 4))
        
        try:
            response = self.scraper.post(api_url, data=data, timeout=30)
            print(f"APIçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                print("âœ… APIè¯·æ±‚æˆåŠŸ")
                
                # å¤„ç†å“åº”å†…å®¹
                content = None
                content_encoding = response.headers.get('Content-Encoding', '').lower()
                
                # æ ¹æ®ç¼–ç ç±»å‹è§£å‹
                if 'br' in content_encoding:
                    print("ğŸ” æ£€æµ‹åˆ°Brotliå‹ç¼©ï¼Œæ­£åœ¨è§£å‹...")
                    try:
                        content = brotli.decompress(response.content).decode('utf-8')
                        print("âœ… Brotliè§£å‹æˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ Brotliè§£å‹å¤±è´¥: {str(e)}")
                        content = response.text
                elif 'gzip' in content_encoding:
                    print("ğŸ” æ£€æµ‹åˆ°gzipå‹ç¼©ï¼Œæ­£åœ¨è§£å‹...")
                    try:
                        compressed_data = io.BytesIO(response.content)
                        with gzip.GzipFile(fileobj=compressed_data) as gzip_file:
                            content = gzip_file.read().decode('utf-8')
                        print("âœ… gzipè§£å‹æˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ gzipè§£å‹å¤±è´¥: {str(e)}")
                        content = response.text
                else:
                    content = response.text
                    print("ğŸ“„ æ— å‹ç¼©æˆ–æœªçŸ¥å‹ç¼©æ ¼å¼")
                
                # è§£æJSONå“åº”
                try:
                    citation_data = json.loads(content)
                    print("âœ… æˆåŠŸè§£æJSONå“åº”")
                    return citation_data
                except Exception as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {str(e)}")
                    return None
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {str(e)}")
            return None
    
    def extract_paper_info(self, citation_data):
        """ä»å¼•ç”¨æ•°æ®ä¸­æå–è®ºæ–‡ä¿¡æ¯"""
        print("ğŸ” ä»å¼•ç”¨æ•°æ®ä¸­æå–è®ºæ–‡ä¿¡æ¯...")
        
        if not citation_data or 'items' not in citation_data:
            print("âŒ æ— æ•ˆçš„å¼•ç”¨æ•°æ®")
            return []
        
        paper_info_list = []
        items = citation_data['items']
        
        print(f"  å¤„ç† {len(items)} ä¸ªæ¡ç›®...")
        
        for item in items:
            # æ¯ä¸ªitemæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯DOI
            for doi, paper_data in item.items():
                try:
                    # æå–åŸºæœ¬ä¿¡æ¯
                    title = paper_data.get('title', 'Unknown Title')
                    
                    # æå–ä½œè€…
                    authors = paper_data.get('author', [])
                    author_names = []
                    for author in authors:
                        if 'family' in author and 'given' in author:
                            author_names.append(f"{author['given']} {author['family']}")
                        elif 'literal' in author:
                            author_names.append(author['literal'])
                    
                    # æå–å…¶ä»–ä¿¡æ¯
                    url = f"{self.base_url}/doi/{doi}"
                    pdf_url = f"{self.base_url}/doi/pdf/{doi}"
                    abstract = paper_data.get('abstract', '')
                    
                    # åˆ›å»ºè®ºæ–‡ä¿¡æ¯å­—å…¸
                    paper_info = {
                        'title': title,
                        'doi': doi,
                        'url': url,
                        'authors': author_names,
                        'pdf_url': pdf_url,
                        'abstract': abstract,
                        'publisher': paper_data.get('publisher', ''),
                        'isbn': paper_data.get('ISBN', ''),
                        'pages': paper_data.get('page', ''),
                        'keywords': paper_data.get('keyword', '')
                    }
                    
                    paper_info_list.append(paper_info)
                except Exception as e:
                    print(f"  âš ï¸ å¤„ç†æ¡ç›®æ—¶å‡ºé”™: {str(e)}")
                    continue
        
        print(f"âœ… æå–åˆ° {len(paper_info_list)} ä¸ªè®ºæ–‡ä¿¡æ¯")
        return paper_info_list